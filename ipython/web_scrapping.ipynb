{
 "metadata": {
  "name": "",
  "signature": "sha256:637725139f750ef53275378358d739e8403a225728d6b7bf4f9909d5da4796cc"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Data Scrapping"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Central to differentiating our process from existing examples of neighborhood ranking systems was the implementation interpretations based on social media and more dynamic datasets. \n",
      "\n",
      "The following set of functions were divided in a number of individual, executable python scripts that were run for over three weeks on a server/vm at MIT. The scripts were used to scrape data from social media and open web APIs. The scripts also parsed the responses obtained from the APIs into readable dictionaries that made it easier to obtain the desired information for the project. \n",
      "\n",
      "Finally, the dictionaries were stored on the server, and later uploaded to github for file management. "
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Code for scrapping trending venues on Foursquare"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The following code hits foursquare's trending endpoint every 20 seconds for very long period of time. Following the response from the server, the data is parsed into a useful format and then saved into a json dictionary. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os\n",
      "import json\n",
      "import time\n",
      "import sys\n",
      "from datetime import datetime\n",
      "import foursquare"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Set the API keys\n",
      "client_id = 'W5UZGTZGO2TJELFWQF1IPYWJ2UXX1WEY2FFZBS14QLXOBKS1'\n",
      "client_secret = 'C2RXMQINBN3ZAOBX3QIOBTYVGXDGWYTPRO5GKNWL0AOC4T12'\n",
      "redirect = 'http://example.com'\n",
      "\n",
      "# set the total time we will be hitting the API per individual json\n",
      "total_time = 160 "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# function that constructs a client object and returns a response from the api\n",
      "def get_checkins(ll):\n",
      "    # Construct the client object\n",
      "    client = foursquare.Foursquare(client_id=client_id, client_secret=client_secret, redirect_uri=redirect)\n",
      "    # get the response from the API\n",
      "    response = client.venues.trending(params={'near': ll, 'limit':'1000', 'radius': '7500'})\n",
      "    return response\n",
      "\n",
      "# function that gets responses from the foursquare api, and parses the data into a useful format\n",
      "def get_many_checkins(lls, total_time, names):\n",
      "    \n",
      "    # empty dict for all the trending venues\n",
      "    all_checkins = {}\n",
      "    remaining_seconds = total_time\n",
      "    interval = 20\n",
      "\n",
      "    while remaining_seconds > 0:\n",
      "        added = 0\n",
      "        for nid, ll in enumerate(lls):\n",
      "            print 'scrapping: ', names[nid]\n",
      "            # get a response from the api\n",
      "            t = get_checkins(ll)\n",
      "            new_checkins = t['venues']\n",
      "            for checkin in new_checkins:\n",
      "                check_id = checkin['id']\n",
      "                if check_id not in all_checkins:\n",
      "                    # parse every venue into a useful dictionary\n",
      "                    properties = {}\n",
      "                    \n",
      "                    properties['content'] = checkin['name']\n",
      "                    properties['here_now'] = checkin['hereNow']['count']\n",
      "                    properties['checkins'] = checkin['stats']['checkinsCount']\n",
      "                    properties['category'] = checkin['categories'][0]['name']\n",
      "                    properties['lat'] = checkin['location']['lat']\n",
      "                    properties['lon'] = checkin['location']['lng']\n",
      "                    properties['checkins'] = checkin['stats']['checkinsCount']\n",
      "                    properties['users_count'] = checkin['stats']['usersCount']\n",
      "                    properties['place_id'] = check_id\n",
      "                    properties['data_point'] = 'none'\n",
      "                    properties['raw_source'] = checkin\n",
      "                    properties['time'] = str(datetime.now())\n",
      "                    properties['data_source'] = 'foursquare_trending'\n",
      "                    \n",
      "                    all_checkins[check_id] = properties\n",
      "                    added += 1\n",
      "        print \"At %d seconds, added %d new checkins, for a total of %d\" % ( total_time - remaining_seconds, added, len(all_checkins))\n",
      "        \n",
      "        # wait some time to avoid going pass the limit of the API\n",
      "        time.sleep(interval)\n",
      "        remaining_seconds -= interval\n",
      "    return all_checkins\n",
      "    \n",
      "# functions that writes a dictionary of trending venues into a json file\n",
      "def run(lls, names):\n",
      "    checkins = get_many_checkins(lls, total_time, names)\n",
      "    name =  str(datetime.now()).replace(\" \", \"_\")\n",
      "    name = name.replace('.', '_')\n",
      "    name = name.replace(':', '_')\n",
      "    \n",
      "    with open( 'foursquare/%sfour_trending.json' %(name), 'w' ) as f:\n",
      "        f.write(json.dumps(checkins))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We are sequentially obtaining trending venues from 5 different boroughs comprising NYC."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# locations for five boroughs \n",
      "lls = ['40.7127, -74.0059', '40.634525, -73.945806', '40.608628, -74.086612', '40.703947, -73.84949', '40.830848, -73.916423']\n",
      "# names of the 5 buroughs to add to the file name\n",
      "names = ['NY', 'Brooklyn', 'Staten Island', 'Queens', 'Bronx']\n",
      "\n",
      "all_time = 9000000000000000000\n",
      "while all_time > 0:\n",
      "    try:\n",
      "        run(lls, names)\n",
      "        all_time -= 30\n",
      "    except: pass"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Code for scrapping different venue types in NYC from Foursquare"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The following code calls foursquare's explore endpoint for each one of their venue types. In order to bypass the 50 venue return limit of the API, a grid of locations within a rectangular area comprising NY is constructed and used to sequentially obtain venues. Following the response from the server, the data is parsed into a useful format and then saved into a json file. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# setup some client variables\n",
      "client_id = 'W5UZGTZGO2TJELFWQF1IPYWJ2UXX1WEY2FFZBS14QLXOBKS1'\n",
      "client_secret = 'C2RXMQINBN3ZAOBX3QIOBTYVGXDGWYTPRO5GKNWL0AOC4T12'\n",
      "redirect = 'http://example.com'\n",
      "\n",
      "# a list of all the venue types to query\n",
      "queries = ['food', 'drinks', 'coffee', 'shops', 'arts', 'outdoors', 'sights', 'specials', 'topPicks']\n",
      "\n",
      "# Construct the client object\n",
      "client = foursquare.Foursquare(client_id=client_id, client_secret=client_secret, redirect_uri=redirect)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# function to construct a decimal range\n",
      "def drange(start, stop, step):\n",
      "    r = start\n",
      "    while r < stop:\n",
      "        yield r\n",
      "        r += step\n",
      "\n",
      "# function to obtain a response from the API based on a set of parameters\n",
      "def get_checkins(ll, query):\n",
      "    # Assign a search radius\n",
      "    rad = '150'\n",
      "    # get the response from the API\n",
      "    response = client.venues.explore(params={'ll': ll, 'radius': rad, 'section': query, 'limit': '1000000'})\n",
      "    return response\n",
      "\n",
      "# function to parse an API response into a useful dictionary\n",
      "def get_venue(check_id):\n",
      "    # Get client response\n",
      "    r = client.venues(check_id)\n",
      "    #print r\n",
      "    # Construct an empty dictionary for the properties\n",
      "    properties = {}\n",
      "    # Get the information of the venue\n",
      "    venue = r['venue']\n",
      "    # Add keys to the dictionary\n",
      "    properties['place_id'] = check_id\n",
      "    properties['name'] = venue['name']\n",
      "    properties['user'] = [item['user']['id'] for item in venue['tips']['groups'][0]['items']]\n",
      "    properties['checkins'] = venue['stats']['checkinsCount']\n",
      "    properties['users_count'] = venue['stats']['usersCount']\n",
      "    properties['category'] = venue['categories'][0]['name']\n",
      "    properties['latitude'] = venue['location']['lat']\n",
      "    properties['longitude'] = venue['location']['lng']\n",
      "    try: properties['rating'] = venue['rating']\n",
      "    except: properties['rating'] =  ''\n",
      "    try: properties['hours'] = venue['hours']['timeframes']\n",
      "    except: properties['hours'] = ''\n",
      "    try: properties['twitter'] = venue['contact']['twitter']\n",
      "    except: properties['twitter'] = ''\n",
      "    try: properties['popular'] = venue['popular']['timeframes']\n",
      "    except: properties['popular'] = ''\n",
      "    properties['time'] = str(datetime.now())\n",
      "    properties['data_source'] = 'foursquare_explore'\n",
      "    \n",
      "    # return the dictionary of properties\n",
      "    return properties\n",
      "\n",
      "# since the api only returns up to 50 places at the time,\n",
      "# we construct a series of locations, and loop through them to scrape it\n",
      "def crawl_4square(query):\n",
      "    we_range = list(drange(-74.290503, -73.702553, .003)) # 50.5 mts radius\n",
      "    sn_range = list(drange(40.482003, 40.918004, .003)) # 50.5 mts radius # 113.8352\n",
      "    print len(sn_range), len(we_range)\n",
      "    # for every latitude\n",
      "    for n in sn_range[134:]:\n",
      "        # for every longitude\n",
      "        for e in we_range[30:150]:\n",
      "            # construct a location\n",
      "            ll = str(n) + ', ' + str(e)\n",
      "            print ll\n",
      "            \n",
      "            try:\n",
      "                # Get all the checkins for a given location\n",
      "                t = get_checkins(ll, query)\n",
      "                new_checkins = t['groups'][0]['items']\n",
      "                all_checkins = {}\n",
      "                # for every checkin in the checkinsggggg\n",
      "                for checkin in new_checkins:\n",
      "                    # Get the venue ID\n",
      "                    check_id = checkin['venue']['id']\n",
      "                    # if the id is not in the dictionary, add it\n",
      "                    if check_id not in all_checkins:\n",
      "                        all_checkins[check_id] = get_venue(check_id)\n",
      "                #print len(all_checkins)\n",
      "                # write the json to a file\n",
      "                if len(all_checkins) > 0:\n",
      "                    print 'places added: ', len(all_checkins)\n",
      "                    with open( 'foursquare_explore/%s/%s,%sfour_explore.json' %(query, str(sn_range.index(n)), str(we_range.index(e))), 'w' ) as f:\n",
      "                        f.write(json.dumps(all_checkins))\n",
      "            except: pass"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# geographical boundaries of NYC\n",
      "sw = '22.1538, 113.8352'\n",
      "ne = '22.5622, 114.4416'\n",
      "sw = '-74.290503, 40.482003'\n",
      "ne = '-73.702553, 40.918004'\n",
      "\n",
      "# Construct a dictionary for all the checkins\n",
      "for query in queries:\n",
      "    querying = crawl_4square(query)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Code for scrapping recent tweets"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The following code calls twitter's search endpoint to obtain recent tweets from a given geographical location. The API is called repeatedly to compile a single dictionary of unique tweets. Following the response from the server, the data is parsed into a useful format and then saved into a json file. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from datetime import datetime\n",
      "from twython import Twython\n",
      "\n",
      "# some api values\n",
      "APP_KEY = '5g8MCsu7a2e74JRORQ22G76uy'\n",
      "APP_SECRET = 'qcwgzUGzgELMFRHQI1tZqsZtvkTTbQxp7KUnjQnR3WjKMin0Ff'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Fetches tweets with a given query at a given lat-long.\n",
      "def get_tweets( latlong=None ):\n",
      "    twitter = Twython( APP_KEY, APP_SECRET )\n",
      "    results = twitter.search( geocode=','.join([ str(x) for x in latlong ]) + ',15km', result_type='recent', count=100 )\n",
      "    return results['statuses']\n",
      "\n",
      "# repeatedly calls the twitter API\n",
      "def get_lots_of_tweets( latlongs, names):\n",
      "    all_tweets = {}\n",
      "    # total time to used to fetch the api for each json file\n",
      "    total_time = 160\n",
      "    remaining_seconds = total_time\n",
      "    interval = 20 \n",
      "    while remaining_seconds > 0:\n",
      "        added = 0\n",
      "        new_tweets = []\n",
      "        for nid, latlong in enumerate(latlongs):\n",
      "            print 'scrapping: ', names[nid]\n",
      "            new_tweets.extend(get_tweets( latlong ))\n",
      "        # parse the tweet response into a dictionary\n",
      "        for tweet in new_tweets:\n",
      "            tid = tweet['id']\n",
      "            if tid not in all_tweets and tweet['coordinates'] != None:\n",
      "                properties = {}\n",
      "                properties['lat'] = tweet['coordinates']['coordinates'][0]\n",
      "                properties['lon'] = tweet['coordinates']['coordinates'][1]\n",
      "                properties['tweet_id'] = tid\n",
      "                properties['content'] = tweet['text']\n",
      "                properties['user'] = tweet['user']['id']\n",
      "                properties['user_location'] = tweet['user']['location']\n",
      "                properties['raw_source'] = tweet\n",
      "                properties['data_point'] = 'none'\n",
      "                properties['time'] = tweet['created_at']\n",
      "                all_tweets[ tid ] = properties\n",
      "                added += 1\n",
      "        print \"At %d seconds, added %d new tweets, for a total of %d\" % (total_time - remaining_seconds, added, len( all_tweets ) )\n",
      "        # wait some time to avoid going over the API rate\n",
      "        time.sleep(interval)\n",
      "        remaining_seconds -= interval\n",
      "    return all_tweets\n",
      "\n",
      "# write a number of tweet dictionaries into a json file\n",
      "def run(locations, names):\n",
      "    t = get_lots_of_tweets(locations, names)\n",
      "    name =  str(datetime.now()).replace(\" \", \"_\")\n",
      "    name = name.replace('.', '_')\n",
      "    name = name.replace(':', '_')\n",
      "    with open('twitter/%stweets.json' %(name), 'w' ) as f:\n",
      "        f.write( json.dumps(t))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# define the location centroid for each borough\n",
      "locations = [[40.7127, -74.0059], [40.634525, -73.945806], [40.608628, -74.086612], [40.703947, -73.84949], [40.830848, -73.916423]]\n",
      "# names of the boroughs\n",
      "names = ['NY', 'Brooklyn', 'Staten Island', 'Queens', 'Bronx']\n",
      "\n",
      "# fetch the API for a long time\n",
      "all_time = 9000000000000000000\n",
      "while all_time > 0:\n",
      "    try:\n",
      "        run(locations, names)\n",
      "        all_time -= 30\n",
      "    except: pass"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Code for scrapping recent instagram posts"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The following code calls instagrams's locations endpoint to obtain recent posts from a given geographical location. The API is called repeatedly to compile a large number of instagram posts over time. Following the response from the server, the data is written into json file. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import requests\n",
      "import threading\n",
      "\n",
      "\n",
      "# setup api parameters\n",
      "Client_ID = '9056a11deaef47a59abc8647208b58d7'\n",
      "Redirect_URI = 'http://www.example.com'\n",
      "code='0340b48b153c4a4b99fe0f8b328a717a'\n",
      "access_token = '23252351.9056a11.93d34bc129f443cba0adef6de1532f8f'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# costruct an API call with the geographical location as parameters\n",
      "def getfeed():\n",
      "    threading.Timer(120.0, getfeed).start()\n",
      "\n",
      "    t = time.strftime(\"%Y%m%d%H%M\")\n",
      "    # obtain the locations from a file\n",
      "    with open('ID') as f:\n",
      "        ID = f.read().splitlines()\n",
      "    data = [None]*len(ID)\n",
      "    counter = 0\n",
      "    for item in ID[0:15]:\n",
      "        url = 'https://api.instagram.com/v1/locations/'+str(item)+'/media/recent?access_token='+access_token\n",
      "        r = requests.get(str(url))\n",
      "        data[counter] = r.json()\n",
      "\n",
      "        counter += 1\n",
      "    # write the responses into a json file\n",
      "    with open('instagram/feed'+t+'.json','a') as f: \n",
      "        json.dump(data, f)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# scrape recursively \n",
      "try: getfeed()\n",
      "except: pass"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Code for fetching crime data from Trulia's API"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The following code calls Trulia\u2019s crime endpoint to obtain recent crime events from a given geographical location. The crime events are updated on a weekly basis, so the code is manually run every week to fetch new data. Following the response from the server, the data is written into json file."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import urllib2\n",
      "\n",
      "def get_json(request):\n",
      "    response = urllib2.urlopen(request)\n",
      "    return response.read()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Request geoJSON from API\n",
      "api = 'http://tiles.trulia.com/crimes/'\n",
      "ny = 'dr72'+','+'dr5q'+',''dr5x'+','+'dr5p'+','+'dr70'+','+'dr78'+','+'dr5w'+','+'dr5n'+','+'dr5r'\n",
      "\n",
      "#Set a maximum number of crime events by county\n",
      "countyCap = 100000\n",
      "# Set a maximum number of crimes \n",
      "limit = 100000\n",
      "# Get the latest version of crimes\n",
      "version = 'E36_21'\n",
      "# Build a string with the request\n",
      "api_request = api + '?geohashes=' + ny + '&' + 'countyCap=' + str(countyCap) + '&' + 'limit=' + str(limit) + '&' + 'v=' + version\n",
      "\n",
      "# get a dictionary with the function\n",
      "json_dict = get_json(api_request)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 29
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# function that takes a dictionary and parses it for spatial manipulation\n",
      "def parse_trulia(geometry_dict):\n",
      "    # extract a specific value\n",
      "    crime_vals = geometry_dict['properties']\n",
      "    crime_vals['lat'] = geometry_dict['geometry']['coordinates'][0]\n",
      "    crime_vals['lon'] = geometry_dict['geometry']['coordinates'][1]\n",
      "    return crime_vals"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# get a response from the API\n",
      "json_dict = json.loads(json_dict)\n",
      "# Parse the dictionary\n",
      "features_list = json_dict['features']\n",
      "\n",
      "# parse the crime events\n",
      "crime_list = []\n",
      "for feature in features_list:\n",
      "    crime_list.append(parse_trulia(feature))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 30
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# write the parsed crimes into a json file\n",
      "with open( 'trulia/%s_trulia.json' %(str(datetime.now())), 'w' ) as f:\n",
      "    f.write(json.dumps(crime_list))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 31
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Code for parsing responses from the instagram API"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Opens the json files previously created and parses them into a useful and consistent format that is then saved into a csv file to be used in mapping GIS software for spatial manipulation."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from os import listdir\n",
      "from os.path import isfile, join\n",
      "from ttp import ttp\n",
      "import csv"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# get a list of json files\n",
      "insta_path = 'instagram/'\n",
      "insta_jsons = [ f for f in listdir(insta_path) if isfile(join(insta_path,f)) ]\n",
      "\n",
      "# create a new csv file\n",
      "csv_file = open('csv/instagram_csv.csv', 'wt')\n",
      "\n",
      "# add the header to the csv file\n",
      "writer = csv.writer(csv_file)\n",
      "writer.writerow(['pid', 'lat', 'lon', 'time', 'content', 'tags'])\n",
      "\n",
      "# create an empty list of ids, to make sure no posts are duplicated\n",
      "all_ids = []\n",
      "for f in insta_jsons:\n",
      "    # open the json files indvidually\n",
      "    with open('instagram/%s' %(f)) as json_file:\n",
      "        grams = json.load(json_file)\n",
      "        cnt = 0\n",
      "        \n",
      "        # every instagram post gets an individual row on the csv file\n",
      "        for gram in grams:\n",
      "            if gram != None:\n",
      "                if len(gram['data']) != 0:\n",
      "                    # parse the json response\n",
      "                    for data in gram['data']:\n",
      "                        content, image, user, caption, time, iid, lat, lon = '', None, None, None, None, None, None, None\n",
      "                        try: image = data['images']['standard_resolution']['url']\n",
      "                        except: pass\n",
      "                        try: user = data['user']['id']\n",
      "                        except: pass\n",
      "                        try: content = data['caption']['text']\n",
      "                        except: pass\n",
      "                        try: time = data['caption']['created_time']\n",
      "                        except: pass\n",
      "                        try: iid = data['id']\n",
      "                        except: pass\n",
      "                        try: lat = data['location']['latitude']\n",
      "                        except: pass\n",
      "                        try: lon = data['location']['longitude']\n",
      "                        except: pass\n",
      "                        raw_source = data\n",
      "                        \n",
      "                        # extract the individual hashtags\n",
      "                        if lat != None and lon != None:\n",
      "                            if iid not in all_ids:\n",
      "                                p = ttp.Parser()\n",
      "                                hashtags = None\n",
      "                                parsed_gram = p.parse(content)\n",
      "                                hashtags = [tag.encode('utf-8') for tag in parsed_gram.tags]\n",
      "                                writer.writerow([iid, lat, lon, time, content.encode('utf-8'), str(hashtags)])\n",
      "                                \n",
      "                                all_ids.append(iid)\n",
      "                                cnt += 1                        \n",
      "                        \n",
      "    print \"Added %d new grams, for a total of %d\" % (cnt, len(all_ids))                        \n",
      "\n",
      "# close the csv file\n",
      "csv_file.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}