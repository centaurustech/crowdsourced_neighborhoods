{
 "metadata": {
  "name": "",
  "signature": "sha256:d9cd714b27679c8c84dd7352767657d816f65ab5dd00ee707a3d89bfc5494a82"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Social Data Analysis"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "While previous work exists in neighborhood ranking, it has mostly used traditional data and focused on more traditional measures of wellness. Through the study, we argue that a more personalized ranking could be achieved by implementing methods that use non-traditional and dynamic datasets for the measurement of livability in the cities. Ubiquitous computing has allowed us to transform our relationship to place making, enabling the construction of crowd-sourced representations of the city. \n",
      "\n",
      "For 3+ weeks a number of social media and mapping API\u2019s were scrapped, to obtain a dynamic representation of a given timeframe in NYC\u2019s boroughs. Only data with a spatial location was considered. The data set was then analyzed with supervised and unsupervised learning algorithms to obtain the overall sentiments existing in NYC\u2019s neighborhoods, and the trending topics that can be associated with livability in the neighborhoods. Similarly, venue datasets were parsed to classify and count abundance of specific business types per neighborhood. "
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Semantic Analysis"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "With the availability of over 200,000 tweets, foursquare checkins, and instagram posts to analyze, semantic analysis was used to catalog and trace social and their spatial implications in given neighborhoods of NYC. We predicted the values of the data, and created new json files that can then be spatially processed to quantify activity and conditions per neighborhood. "
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Unsupervised Learning: Topic-Modeling"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We used topic models to discover the abstract \"topics\" that occur in a collection of social media posts. The topic model allows examining a set of documents and discovering, based on the statistics of the words in each, what the topics might be and what each dataset\u2019s balance of topics is.\n",
      "\n",
      "We used Latent Dirichlet allocation (LDA), the most common topic model, developed by David Blei, Andrew Ng, and Michael Jordan in 2002. The dataset was divided into a number of subgroups of common topics. The common topics included the most common words used to classify each post; these words were then used to manually classify the groups into common topics of interest for the study of livability in neighborhoods. Finally, the social media posts were assigned topics which were used to quantify occurrences within neighborhoods. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from os import listdir\n",
      "from os.path import isfile, join\n",
      "from ttp import ttp\n",
      "from nltk.corpus import stopwords\n",
      "from gensim import corpora, models, similarities\n",
      "import numpy as np\n",
      "\n",
      "import json\n",
      "import csv"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "/Users/carlos/anaconda/lib/python2.7/site-packages/numpy/lib/utils.py:134: DeprecationWarning: `scipy.sparse.sparsetools` is deprecated!\n",
        "scipy.sparse.sparsetools is a private module for scipy.sparse, and should not be used.\n",
        "  warnings.warn(depdoc, DeprecationWarning)\n",
        "/Users/carlos/anaconda/lib/python2.7/site-packages/scipy/lib/_util.py:35: DeprecationWarning: Module scipy.linalg.blas.fblas is deprecated, use scipy.linalg.blas instead\n",
        "  DeprecationWarning)\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "/Users/carlos/anaconda/lib/python2.7/site-packages/scipy/lib/_util.py:35: DeprecationWarning: Module scipy.linalg.blas.fblas is deprecated, use scipy.linalg.blas instead\n",
        "  DeprecationWarning)\n",
        "/Users/carlos/anaconda/lib/python2.7/site-packages/scipy/lib/_util.py:35: DeprecationWarning: Module scipy.linalg.blas.fblas is deprecated, use scipy.linalg.blas instead\n",
        "  DeprecationWarning)\n",
        "/Users/carlos/anaconda/lib/python2.7/site-packages/scipy/lib/_util.py:35: DeprecationWarning: Module scipy.linalg.blas.fblas is deprecated, use scipy.linalg.blas instead\n",
        "  DeprecationWarning)\n",
        "/Users/carlos/anaconda/lib/python2.7/site-packages/scipy/lib/_util.py:35: DeprecationWarning: Module scipy.linalg.blas.fblas is deprecated, use scipy.linalg.blas instead\n",
        "  DeprecationWarning)\n",
        "/Users/carlos/anaconda/lib/python2.7/site-packages/scipy/lib/_util.py:35: DeprecationWarning: Module scipy.linalg.blas.fblas is deprecated, use scipy.linalg.blas instead\n",
        "  DeprecationWarning)\n",
        "/Users/carlos/anaconda/lib/python2.7/site-packages/scipy/lib/_util.py:35: DeprecationWarning: Module scipy.linalg.blas.fblas is deprecated, use scipy.linalg.blas instead\n",
        "  DeprecationWarning)\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "/Users/carlos/anaconda/lib/python2.7/site-packages/scipy/lib/_util.py:35: DeprecationWarning: Module scipy.linalg.blas.fblas is deprecated, use scipy.linalg.blas instead\n",
        "  DeprecationWarning)\n",
        "/Users/carlos/anaconda/lib/python2.7/site-packages/scipy/lib/_util.py:35: DeprecationWarning: Module scipy.linalg.blas.fblas is deprecated, use scipy.linalg.blas instead\n",
        "  DeprecationWarning)\n",
        "/Users/carlos/anaconda/lib/python2.7/site-packages/scipy/lib/_util.py:35: DeprecationWarning: Module scipy.linalg.blas.fblas is deprecated, use scipy.linalg.blas instead\n",
        "  DeprecationWarning)\n",
        "/Users/carlos/anaconda/lib/python2.7/site-packages/scipy/lib/_util.py:35: DeprecationWarning: Module scipy.linalg.blas.fblas is deprecated, use scipy.linalg.blas instead\n",
        "  DeprecationWarning)\n",
        "/Users/carlos/anaconda/lib/python2.7/site-packages/scipy/lib/_util.py:35: DeprecationWarning: Module scipy.linalg.blas.fblas is deprecated, use scipy.linalg.blas instead\n",
        "  DeprecationWarning)\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Data Selection and Parsing"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The tweets that have been scrapped for over 3 weeks are loaded from their respective json files. We make sure that the tweets are unique and not repeated. \n",
      "\n",
      "After a number of tests to decide on what information from the tweets was relevant to our study, we decided to do the topic-modeling analysis based on hashtags of tweets. \n",
      "\n",
      "The first test involved analyzing the actual tweet itself, but this resulted in a lot of noise due to emoticons, non-English words, etc. \n",
      "\n",
      "For a second test we stripped off non-English words and excessive punctuation to clean the tweets. This experiment didn\u2019t yield the best distribution of topic groups, since a lot of the meaning in the tweets doesn\u2019t seem to be encoded on the English words themselves.\n",
      "\n",
      "For a third test, the hashtags were analyzed, yielding more relevant information and topic groups. However, we noticed that hashtags related to NYC were adding noise to the dataset, and didn\u2019t contribute with additional information. We lastly deleted hashtags that were close to NYC. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# hashtags related to NY are eliminated, since they add noise to the topic modelling analysis and don't contribute to our purpose\n",
      "new_yorks = ['nyc', 'newyork', 'manhattan', 'brooklin', 'new_york', 'new york', 'brooklyn', 'ny', 'newyorkcity', 'nuevayork', 'bk']\n",
      "\n",
      "# function that opens twitter parsed jsons on a given file path, and returns their hashtags, or a list of location, content, and ID                            \n",
      "def jsons_to_mm_tuple(twi_path, just_hashes=False):\n",
      "    # make sure only filenames are on the list of files\n",
      "    twi_jsons = [ f for f in listdir(twi_path) if isfile(join(twi_path,f))]#[:100]\n",
      "    \n",
      "    # initialize an empty list to store tweet ids\n",
      "    all_ids = []\n",
      "    for f in twi_jsons:\n",
      "        # open each json file\n",
      "        with open('%s%s' %(twi_path, f)) as json_file:\n",
      "            # load the json into a python dict\n",
      "            try:\n",
      "                tweets = json.load(json_file).values()\n",
      "                for tweet in tweets:\n",
      "                    content = tweet['content']\n",
      "                    p = ttp.Parser()\n",
      "                    # parse the content for hashtags: returns a list of hashtags\n",
      "                    parsed_tweet = p.parse(content)\n",
      "                    # hashtags should be lower case, and not include newyork or similar hastags\n",
      "                    hashtags = [tag.lower().encode('utf-8') for tag in parsed_tweet.tags if tag.lower().encode('utf-8') not in new_yorks]\n",
      "\n",
      "                    # we only process the jsons which actually have hashtags and geographical coordinates\n",
      "                    if len(hashtags) > 0:\n",
      "                        if float(tweet['lat']) != 0 and float(tweet['lon']) != 0:\n",
      "                            # we make sure that the tweets are not already in our list of tweets\n",
      "                            if tweet['tweet_id'] not in all_ids:\n",
      "                                # add the new tweet to the list of tweets\n",
      "                                all_ids.append(tweet['tweet_id'])\n",
      "\n",
      "                                # if we want to yield more information than just the hashtags\n",
      "                                if not just_hashes:\n",
      "                                    yield [tweet['tweet_id'], tweet['lat'], tweet['lon'], content, hashtags]\n",
      "                                else: \n",
      "                                    yield hashtags\n",
      "            except: pass"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# extract the tweets from the jsons\n",
      "twi_path = '../python/twitter/'\n",
      "tweets = jsons_to_mm_tuple(twi_path, True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Data Wrangling"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A list of hashtags per tweet is used to build a corpora dictionary on the Gensim library. Additionally, stop words are removed from the dictionary, and hashtags that appear only once on the dataset. Finally, the corpora dictionary is compactified. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# TOKENIZERATOR: collect statistics about all tokens\n",
      "dictionary = corpora.Dictionary(tweets)\n",
      "\n",
      "dictionary"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 4,
       "text": [
        "<gensim.corpora.dictionary.Dictionary at 0x18012588>"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# get a list of stop words from the nltk library\n",
      "stoplist = stopwords.words('english')\n",
      "\n",
      "# DICTERATOR: remove stop words and words that appear only once \n",
      "stop_ids = [dictionary.token2id[stopword] for stopword in stoplist if stopword in dictionary.token2id]\n",
      "once_ids = [tokenid for tokenid, docfreq in dictionary.dfs.iteritems() if docfreq == 1]\n",
      "\n",
      "# filter the tokens from the corpora dict\n",
      "dictionary.filter_tokens(stop_ids + once_ids)\n",
      "dictionary"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 5,
       "text": [
        "<gensim.corpora.dictionary.Dictionary at 0x18012588>"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# remove gaps in id sequence after words that were removed\n",
      "dictionary.compactify() \n",
      "dictionary"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 6,
       "text": [
        "<gensim.corpora.dictionary.Dictionary at 0x18012588>"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# given a dictionary and a list of ids, get the words that correspond to the ids\n",
      "def get_singles(dictionary, ids):\n",
      "    for word_id in once_ids:\n",
      "        yield dictionary.get(word_id)\n",
      "\n",
      "# eliminate the words that appear only once        \n",
      "def filter_singles(singles, texts):\n",
      "    for text in texts:\n",
      "        new_list = []\n",
      "        for word in text:\n",
      "            if word not in singles:\n",
      "                new_list.append(word)\n",
      "        yield new_list      "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "LDA model"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A bag-of-words is created with the hashtags. While the corpora dictionary was cleaned with the functions of the dictionary class, a number of functions were developed to clean the lists of hashtags efficiently to be used for the creation of the bag-of-words. \n",
      "\n",
      "A LDA model was chosen to classify the tweets. The unsupervised learning process involved the use of over 150,000+ tweets with spatial information. Once the LDA model was trained, different numbers of groups were used to choose an optimal distribution of topics. A very reduced number of topic groups would tend to underfit the data, while a large number of topic groups would tend to overfit the data. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "singles = get_singles(dictionary, once_ids)\n",
      "filtered_texts = filter_singles(singles, jsons_to_mm_tuple(twi_path, True))\n",
      "\n",
      "# Create Bag of words\n",
      "mm = [dictionary.doc2bow(text) for text in filtered_texts] "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# define the number of topics for the classification\n",
      "num_topics = 40\n",
      "\n",
      "# Trains the LDA models with the corpus and dictionary previously created\n",
      "lda = models.ldamodel.LdaModel(corpus=list(mm), id2word=dictionary, num_topics=num_topics, \n",
      "                               update_every=1, chunksize=10000, passes=10, iterations=50)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# prints all groups and their main words\n",
      "lda.print_topics(num_topics=num_topics, num_words=15)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 10,
       "text": [
        "[u'0.069*iloveny + 0.061*turkey + 0.057*mentionsomeonebeautiful + 0.048*tge + 0.028*beautiful + 0.026*lol + 0.021*rain + 0.021*les + 0.019*pie + 0.018*city + 0.016*apple + 0.016*fidi + 0.012*immigrationaction + 0.011*snowing + 0.010*quote',\n",
        " u'0.057*nowplaying + 0.047*scandal + 0.047*knicks + 0.023*soulfulflow + 0.023*sundaesermonradio + 0.019*happiness + 0.017*thewalkingdead + 0.014*voiceresults + 0.012*jetblue + 0.012*foreverwithbap + 0.011*followtrick + 0.011*happythanksgivingeveryone + 0.010*911memorial + 0.010*villanova + 0.009*travelhappy',\n",
        " u'0.437*thanksgiving + 0.148*macysthanksgivingdayparade + 0.027*queens + 0.018*beermenus + 0.015*newyorkdiaries + 0.015*unionsquare + 0.013*regram + 0.012*amazing + 0.011*bgcredemption + 0.010*mentionsomeoneyouarethankfulfor + 0.010*tdaycbs + 0.010*ootd + 0.007*fail + 0.007*bgc13 + 0.006*mcm',\n",
        " u'0.107*family + 0.044*november + 0.040*thanksgiving_edition + 0.035*macysdayparade + 0.027*imthankfulfor + 0.027*mentionsomeoneyourethankfulfor + 0.023*peopleimthankfulfor + 0.021*highliteswithik + 0.019*nets + 0.017*morning + 0.016*nfl + 0.012*us + 0.012*foxnews + 0.011*cocktails + 0.011*cats',\n",
        " u'0.074*snow + 0.055*christmas + 0.050*fall + 0.032*shoutout + 0.030*gobblegobble + 0.020*makeup + 0.019*realtalk + 0.016*winter + 0.014*homeland + 0.014*terminal5 + 0.012*snl + 0.012*hair + 0.012*realshit + 0.011*interpol + 0.011*santa',\n",
        " u'0.055*soho + 0.047*photography + 0.036*nyr + 0.019*chinatown + 0.019*dmargeniis_x3 + 0.018*video + 0.017*happybirthday + 0.015*kiss + 0.013*afterlight + 0.012*usa + 0.012*yonkers + 0.011*selfies + 0.010*thedailyknews + 0.009*leadership + 0.009*deliveringlove',\n",
        " u'0.038*vintage + 0.036*streetart + 0.026*graffiti + 0.023*otrrh + 0.022*commsconsortium14 + 0.021*rockettes + 0.018*lunch + 0.017*grandcentral + 0.016*nba + 0.014*girls + 0.013*sexy + 0.012*lhhh + 0.011*hungergames + 0.009*lic + 0.009*ericgarner',\n",
        " u'0.052*turkeyday + 0.030*lifestylethursdays + 0.029*work + 0.028*gratitude + 0.020*peace + 0.019*diygltour + 0.019*thediytour + 0.018*saturday + 0.018*beauty + 0.017*rhoa + 0.015*chelsea + 0.014*pacquiaoalgieri + 0.014*newjersey + 0.013*sapsmesummit + 0.012*edm',\n",
        " u'0.079*blessed + 0.073*music + 0.036*america + 0.031*williamsburg + 0.031*foodporn + 0.024*hiphop + 0.023*new + 0.017*rn + 0.017*usa + 0.017*ilovenyc + 0.016*longisland + 0.013*taylorswift + 0.011*followforfollow + 0.009*facebook + 0.009*science',\n",
        " u'0.169*tbt + 0.076*selfie + 0.024*sorrynotsorry + 0.023*fb + 0.021*empirestatebuilding + 0.020*timesquare + 0.016*empirestate + 0.013*obama + 0.013*throwbackthursday + 0.012*bryantpark + 0.010*bigapple + 0.009*ramen + 0.007*americanfield + 0.007*latte + 0.007*money',\n",
        " u'0.109*mikebrown + 0.057*holiday + 0.028*handsupdontshoot + 0.024*nypd + 0.022*letsgovcu + 0.021*nojusticenopeace + 0.019*freedomtower + 0.015*throwback + 0.014*free + 0.013*protest + 0.013*westvillage + 0.012*icecream + 0.011*tistheseason + 0.010*mlisnyc + 0.010*mikebrownverdict',\n",
        " u'0.090*photo + 0.076*statueofliberty + 0.052*goodmorning + 0.047*htgawm + 0.032*subway + 0.027*bronx + 0.027*mentionpeopleimthankfulfor + 0.023*mta + 0.021*ff + 0.016*hotplayradio10pm + 0.016*hotfriday + 0.016*whyimthankful + 0.011*dancilate + 0.011*train + 0.010*tuesday',\n",
        " u'0.209*jobs + 0.208*job + 0.129*tweetmyjobs + 0.061*clerical + 0.038*marketing + 0.030*salute + 0.021*coffee + 0.021*voicesaveryan + 0.019*businessmgmt + 0.018*admin + 0.014*skilledtrade + 0.009*turnup + 0.009*opensource + 0.007*rt + 0.006*boxing',\n",
        " u'0.159*blackfriday + 0.031*birthday + 0.018*ny2014 + 0.018*shopping + 0.015*timeless + 0.013*fitfam + 0.013*pumpkin + 0.012*jazz + 0.012*burlesque + 0.011*funny + 0.011*bday + 0.011*mexico + 0.011*uws + 0.010*sunrise + 0.010*thash',\n",
        " u'0.112*amas + 0.088*repost + 0.070*timessquare + 0.056*amas2014 + 0.054*nofilter + 0.025*dinner + 0.021*vegan + 0.021*newark + 0.020*sale + 0.018*bucketlist + 0.016*kinusselfie + 0.012*billcosby + 0.010*nyg + 0.009*disney + 0.008*wedding',\n",
        " u'0.028*aaronandbrentcollab + 0.025*classic + 0.023*secaucus + 0.022*tweetandrepeatsweeps + 0.019*cbtjewelry + 0.018*mentionpeopleyourethankfulfor + 0.018*duro + 0.017*dessert + 0.017*apoya + 0.013*talktomematt + 0.013*digitaldiplomacy + 0.011*willard71 + 0.010*transportation + 0.010*truthtweetedontotaltoptuesday + 0.010*ikede',\n",
        " u'0.135*thankful + 0.040*grateful + 0.027*theprofit + 0.019*cowboys + 0.018*giants + 0.018*gotham + 0.016*cake + 0.014*friday + 0.012*upperwestside + 0.011*realestate + 0.011*lamercriminals + 0.009*streetphotography + 0.008*football + 0.008*lowereastside + 0.008*byefelicia',\n",
        " u'0.042*usa + 0.022*tamirrice + 0.020*thanksgivingparade + 0.017*studiosquare + 0.014*bridge + 0.012*svengoolie + 0.012*magical + 0.012*believe + 0.011*flexandshanice + 0.009*airport + 0.009*jurassicworld + 0.009*restaurant + 0.009*cheese + 0.008*startrek + 0.008*luchaunderground',\n",
        " u'0.430*happythanksgiving + 0.027*809lounge + 0.027*leruffo + 0.015*gym + 0.014*midtown + 0.013*yummy + 0.011*workout + 0.010*health + 0.010*bts + 0.010*wasonbrazoban + 0.009*happyholidays + 0.009*dreambig + 0.009*tagsforlikes + 0.008*dreamcrea + 0.008*fit',\n",
        " u'0.044*mtvstars + 0.025*soul + 0.024*breakfast + 0.021*jfk + 0.021*awesome + 0.021*thankyou + 0.020*autumn + 0.014*lfc + 0.011*dumbo + 0.010*clemsonfamily + 0.010*worldtradecenter + 0.010*macys2014 + 0.010*latingrammy + 0.009*hotel + 0.009*traffic',\n",
        " u'0.092*thanksgivingeve + 0.072*stoptheparade + 0.037*education + 0.032*appletv + 0.026*smh + 0.022*micheillesoifermujertalentosa + 0.021*aerospace + 0.019*teterboro + 0.018*oomf + 0.015*thanksgivingbreak + 0.013*castro + 0.013*nbc + 0.011*fullyequippedent + 0.010*handmade + 0.009*livefeed',\n",
        " u'0.089*vscocam + 0.047*moma + 0.038*fun + 0.037*architecture + 0.029*vsco + 0.025*wtc + 0.016*design + 0.013*weather + 0.012*snapchat + 0.012*spongebob + 0.011*astoria + 0.010*balloons + 0.010*squad + 0.009*united + 0.009*movie',\n",
        " u'0.294*macysparade + 0.048*camfollowme + 0.018*followmekiki + 0.017*jackdailto400k + 0.016*artravelivestream + 0.016*voicesaveanita + 0.015*statenisland + 0.014*vaynerthanks + 0.014*voicesavereagan + 0.012*fergusonnyc + 0.012*turkeytrot + 0.011*film + 0.009*madisonscouts + 0.009*followmenash + 0.009*trashtue',\n",
        " u'0.083*friends + 0.070*food + 0.067*parade + 0.029*yum + 0.029*goodtimes + 0.024*cowboysnation + 0.023*follow + 0.019*beer + 0.016*weekend + 0.014*foodporn + 0.014*fun + 0.013*goals + 0.013*happyhour + 0.012*blessings + 0.012*glutenfree',\n",
        " u'0.042*facts + 0.030*contest + 0.030*letfizziein + 0.028*foodie + 0.025*instagram + 0.025*instagood + 0.019*spiderman + 0.018*tgit + 0.017*lga + 0.017*theview + 0.016*knickstape + 0.014*organic + 0.013*random + 0.013*instamood + 0.012*fbf',\n",
        " u'0.077*fashion + 0.040*home + 0.037*friendsgiving + 0.034*style + 0.026*kinus + 0.025*model + 0.022*givethanks + 0.020*menswear + 0.014*photoshoot + 0.013*ncaab + 0.012*hnachildhoodmemories + 0.010*inthahood + 0.010*photooftheday + 0.008*killerheels + 0.007*quotes',\n",
        " u'0.043*justiceformikebrown + 0.029*tgif + 0.027*dwtsfinale + 0.026*night + 0.026*interpol + 0.021*monday + 0.018*cheers + 0.018*lights + 0.016*nightlife + 0.015*workflow + 0.013*tittiesandwingswednesdays + 0.012*bestdayever + 0.012*handsup + 0.012*god + 0.012*duke',\n",
        " u'0.227*job + 0.145*jobs + 0.080*tweetmyjobs + 0.051*nursing + 0.048*jerseycity + 0.046*sales + 0.033*retail + 0.028*veteranjob + 0.028*customerservice + 0.016*healthcare + 0.013*belleville + 0.013*nowhiring + 0.011*interpreter + 0.011*bilingual + 0.007*automotive',\n",
        " u'0.112*blacklivesmatter + 0.078*fergusondecision + 0.045*sunset + 0.034*scandai + 0.030*fergsuon + 0.025*ferguson2nyc + 0.024*shutitdown + 0.012*xmas + 0.012*tribeca + 0.011*tb + 0.010*ijustneedaminute + 0.010*omg + 0.009*luxury + 0.008*batman + 0.008*jimmyfallon',\n",
        " u'0.061*travel + 0.030*harlem + 0.021*rip + 0.020*black + 0.019*fhstakesnyc + 0.016*inspiration + 0.014*red + 0.014*legend + 0.013*fitness + 0.013*jacciandjimmyintheusa + 0.012*beyonce + 0.011*winteriscoming + 0.010*scandalabc + 0.010*saturdaynight + 0.010*adventuretime',\n",
        " u'0.397*ferguson + 0.045*latergram + 0.031*michaelbrown + 0.021*fergusondecision + 0.015*dwts + 0.015*nycprotest + 0.015*brunch + 0.014*darrenwilson + 0.011*funk + 0.011*alllivesmatter + 0.011*pizza + 0.010*classicrock + 0.010*nyc2ferguson + 0.009*downtown + 0.009*psychedelic',\n",
        " u'0.055*thankfulfor + 0.039*brooklynbridge + 0.038*nj + 0.026*msg + 0.025*eeeeeats + 0.020*macysthanksgivingparade + 0.019*wine + 0.018*mitadymitad + 0.017*hellokitty + 0.014*cute + 0.013*mockingjay + 0.012*muaythai + 0.012*immigration + 0.011*scny + 0.010*comedy',\n",
        " u'0.050*life + 0.035*live + 0.026*flipbookit + 0.024*portrait + 0.023*nailart + 0.020*vanityprojects + 0.018*gelnail + 0.017*gay + 0.017*books + 0.014*handpainted + 0.014*bobov + 0.013*werk + 0.011*whynot + 0.011*newarrivals + 0.010*immigrationreform',\n",
        " u'0.109*art + 0.041*mentionpeopleyouarethankfulfor + 0.037*wwe + 0.032*cold + 0.018*raw + 0.017*artist + 0.015*dance + 0.012*run + 0.011*cabaret + 0.010*columbuscircle + 0.010*artlife + 0.009*dreams + 0.009*artwork + 0.008*lifestyle + 0.008*birdpeople',\n",
        " u'0.042*thanksgiving2014 + 0.031*party + 0.023*studio + 0.022*tonight + 0.011*jesus + 0.011*rhobh + 0.010*light + 0.010*givingthanks + 0.010*college + 0.010*shop + 0.010*instagay + 0.009*jets + 0.009*igers + 0.009*fallontonight + 0.009*bullypharrell',\n",
        " u'0.026*macy + 0.023*housemusic + 0.019*wallstreet + 0.018*rockefellercenter + 0.015*mgwv + 0.014*dog + 0.014*bushwick + 0.014*nature + 0.014*ripmikebrown + 0.012*boycottblackfriday + 0.012*billyjoelmsg + 0.011*cookstoves + 0.011*balloon + 0.010*fml + 0.010*vacaciones2014',\n",
        " u'0.191*love + 0.037*thanksgivingday + 0.037*winter + 0.032*neverforget + 0.030*thanks + 0.027*hoboken + 0.022*eastvillage + 0.020*skyline + 0.017*billyjoel + 0.016*winning + 0.015*blackandwhite + 0.014*followmecam + 0.011*flipagram + 0.010*tcot + 0.008*topoftherock',\n",
        " u'0.074*np + 0.040*soundcloud + 0.019*truth + 0.018*snoopy + 0.015*sunday + 0.013*sundayfunday + 0.012*dalvsnyg + 0.009*datenight + 0.009*mockingjaypart1 + 0.009*eredcarpet + 0.008*smile + 0.008*pretty + 0.008*drink + 0.007*vixxnyc + 0.007*rp',\n",
        " u'0.100*macys + 0.064*wcw + 0.063*thanksgivingdayparade + 0.060*happy + 0.042*holidays + 0.032*survivorseries + 0.023*wtf + 0.017*thevoice + 0.015*delicious + 0.011*applepie + 0.011*nygiants + 0.010*magic + 0.009*twitter + 0.009*street + 0.008*rock',\n",
        " u'0.067*centralpark + 0.063*vacation + 0.058*trip + 0.050*broadway + 0.048*ferias + 0.034*howtogetawaywithmurder + 0.024*playlistlive + 0.023*rivalry150 + 0.013*uptown + 0.012*tradition + 0.012*movember + 0.011*isles + 0.010*pachanyc + 0.010*faizaltravels + 0.009*livemusic']"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Tweet Topic Classification"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The tweets were divided in N number of groups. Each group would be classified based on how related the hashtags are to each other. Additionally, the most common words were used to manually assign a topic based on the most common hashtags. The topic occurrence was also quantified based on the frequency of the words related to that topic. The occurrence rate could be then used to create a weighted average. \n",
      "\n",
      "The manual classification was written on a CSV file with topic type and occurrence percentage per group. For N number of groups, N numbers of rows were written on the CSV file. The topics that were classified from the dataset were: trendy, foodie, nightlife, public space, music, social justice, and fitness. These topics reflect the most relevant categories for our study based on the clustering performed by the LDA model. \n",
      "\n",
      "The tweet dictionaries were then assigned to its specific group, and assigned a topic or topics and occurrence percentages. The new dictionaries including a topic classification were then written into new json files to be spatially analyzed with mapping software. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# get a list of processed topics obtained by training an LDA model, and return them as individual lists of topics and frewuencies\n",
      "def parse_topics(filepath):\n",
      "    with open(filepath, 'rU') as f:\n",
      "        reader = list(csv.reader(f))\n",
      "        header = reader[0]\n",
      "        reader.pop(0)\n",
      "        topics = []\n",
      "        freqs = []\n",
      "        for row in reader:\n",
      "            freq = []\n",
      "            topic = []\n",
      "            row.pop(0)\n",
      "            for ind, element in enumerate(row):\n",
      "                if ind%2 == 0:\n",
      "                    try: \n",
      "                        fr = row[ind+1]\n",
      "                    except: \n",
      "                        fr = ''\n",
      "                    if fr != '':\n",
      "                        topic.append(element)\n",
      "                        freq.append(row[ind+1])\n",
      "            topics.append(topic)\n",
      "            freqs.append(freq)\n",
      "        return topics, freqs      "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# given a corpus trained with the LDA classifier, and a threshold, classify the tweets into the groups \n",
      "def classify(lda_corpus, texts, cluster_num, threshold, words=None, frequencies=None):\n",
      "    for i,j in zip(lda_corpus, texts):\n",
      "        try: \n",
      "            if i[cluster_num][1] > threshold:\n",
      "                j.append(words[cluster_num])\n",
      "                j.append(frequencies[cluster_num])\n",
      "                yield j \n",
      "        except: pass"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Assigns the topics to the documents in corpus\n",
      "lda_corpus = lda[mm]\n",
      "threshold = 1/float(num_topics)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# function that takes the topic classification of a given tweet, and other data of the tweets and writes a new json to be spatially joined\n",
      "def topic_to_json(topic_num, topics, frequencies):\n",
      "    for tweet in classify(lda_corpus, jsons_to_mm_tuple(twi_path), topic_num, threshold, topics, frequencies):\n",
      "        tid, lat, lon, content, hashtags, topic, frequency = tweet\n",
      "        with open('json_topic/%stopic_tweet.json' %(tid), 'w') as f:\n",
      "            f.write( json.dumps({'lat':lat, 'lon':lon, 'tweet':content, 'hashtags':hashtags, 'topic':topic, 'frequency':frequency}))\n",
      "            #print 'wrote tweet %s' %(tid)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "topics, frequencies = parse_topics('twitter_40topic_classification.csv')\n",
      "\n",
      "# for every topic group, write json files for every tweet\n",
      "for topic_num in np.arange(num_topics):#lda_corpus, jsons_to_mm_tuple(twi_path), topic_num, threshold, num_topics):        \n",
      "    topic_to_json(topic_num, topics, frequencies)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Supervised Learning: Sentiment Analysis"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Sentiment analysis was used to determine the overall contextual polarity of a social media post. The attitude may be the judgment or evaluation, affective state, or the intended emotional communication. \n",
      "\n",
      "A series of social media posts were analyzed to obtain a measurement of how negative its content was. We used a Na\u00efve Bayes classifier that was previously trained with over 2,000 document ratings. \n",
      "\n",
      "The process involved extensive cleaning and parsing of the media posts; since a significant part of the posts are not written in English, or refer to words not in the English dictionary, these posts would add significant noise to the study. \n",
      "\n",
      "Finally, after the analysis, the posts were parsed and written into new json files for its posterior spatial analysis. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from os import listdir\n",
      "from os.path import isfile, isdir, join\n",
      "from ttp import ttp\n",
      "\n",
      "import json\n",
      "import csv\n",
      "\n",
      "from textblob import TextBlob\n",
      "from textblob.sentiments import NaiveBayesAnalyzer"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Data Wrangling"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Given a file with 43,000+ eEnglish words compiled, the words in a tweet are evaluated. Each word is reformatted in lower-case, and the punctuation is removed. Only the posts that are mostly written in English are returned. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "'''\n",
      "Code adapted from http://inventwithpython.com/hacking/chapter12.html\n",
      "'''\n",
      "\n",
      " \n",
      "#Functions to parse sentence, making sure most of the content is english and words\n",
      "UPPERLETTERS = 'ABCDEFGHIJKLMNOPQRSTUVWXYZ'\n",
      "LETTERS_AND_SPACE = UPPERLETTERS + UPPERLETTERS.lower() + ' \\t\\n'\n",
      "\n",
      "def loadDictionary():\n",
      "    dictionaryFile = open('dictionary.txt')\n",
      "    englishWords = {}\n",
      "    for word in dictionaryFile.read().split('\\n'):\n",
      "        englishWords[word] = None\n",
      "    dictionaryFile.close()\n",
      "    return englishWords\n",
      "\n",
      "ENGLISH_WORDS = loadDictionary()\n",
      "\n",
      "def getEnglishCount(message):\n",
      "    message = message.upper()\n",
      "    message = removeNonLetters(message)\n",
      "    possibleWords = message.split()\n",
      "\n",
      "    if possibleWords == []:\n",
      "        # no words at all, so return 0.0\n",
      "        return 0.0, ''  \n",
      "\n",
      "    matches = 0\n",
      "    all_words = []\n",
      "    for word in possibleWords:\n",
      "        if word in ENGLISH_WORDS:\n",
      "            matches += 1\n",
      "            all_words.append(word + ' ')\n",
      "    return float(matches) / len(possibleWords), ''.join(all_words).lower()\n",
      "\n",
      "\n",
      "def removeNonLetters(message):\n",
      "    lettersOnly = []\n",
      "    for symbol in message:\n",
      "        if symbol in LETTERS_AND_SPACE:\n",
      "            lettersOnly.append(symbol)\n",
      "    return ''.join(lettersOnly)\n",
      "\n",
      "\n",
      "def isEnglish(message, wordPercentage=20, letterPercentage=85):\n",
      "    # By default, 20% of the words must exist in the dictionary file, and\n",
      "    # 85% of all the characters in the message must be letters or spaces\n",
      "    # (not punctuation or numbers).\n",
      "    cnt, sentence = getEnglishCount(message)\n",
      "    words_match = cnt * 100 >= wordPercentage\n",
      "    numLetters = len(removeNonLetters(message))\n",
      "    messageLettersPercentage = float(numLetters) / len(message) * 100\n",
      "    letters_match = messageLettersPercentage >= letterPercentage\n",
      "    return words_match and letters_match, sentence"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Data Selection and Parsing"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The tweets that have been scrapped for over 3 weeks are loaded from their respective json files. We make sure that the tweets contain English words.\n",
      "\n",
      "After a number of tests to decide on what information from the tweets was relevant to our study, we decided to do the sentiment analysis based on English-only words that are on the social media posts.\n",
      "\n",
      "The first test involved analyzing the hashtags, but since the trained/classified datasets available mostly focus on actual words, and hashtags are oftentimes not words, these approach resulted in inaccurate results.\n",
      "\n",
      "For a second test we stripped off non-English words and excessive punctuation to clean the tweets. This experiment in addition to removing non-English words resulted in the most noise-reduction approach to the study. However, it should be noted that at times, large parts of the posts were eliminated, potentially decreasing the accuracy of the analysis. Future potentials for the study point to the development of a manually classified dataset of terms specific to social media, which could be achieved with the help of crowd-sourcing."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# function that opens twitter parsed jsons on a given file path, and returns a list of location, content, and ID   \n",
      "# the sentence must contain words that are mostly english\n",
      "def jsons_to_tuple(twi_path):\n",
      "    twi_jsons = [ f for f in listdir(twi_path) if isfile(join(twi_path,f)) ][:100]\n",
      "\n",
      "    all_ids = []\n",
      "    for f in twi_jsons:\n",
      "        with open('%s%s' %(twi_path, f)) as json_file:\n",
      "            tweets = json.load(json_file).values()\n",
      "            for tweet in tweets:\n",
      "                content = tweet['content']\n",
      "                \n",
      "                # return only sentences that are english, and return the formatted words of the sentence\n",
      "                is_english, new_content = isEnglish(content, 20, 65)\n",
      "                if is_english:\n",
      "                    if float(tweet['lat']) != 0 and float(tweet['lon']) != 0:\n",
      "                        if tweet['tweet_id'] not in all_ids:\n",
      "                            all_ids.append(tweet['tweet_id'])\n",
      "                            yield tweet['tweet_id'], tweet['lat'], tweet['lon'], new_content"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "twi_path = '../python/twitter/'\n",
      "tweets = jsons_to_tuple(twi_path)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Naive Bayes Analysis"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The parsed texts are analyzed with the textblob.sentiments module of the library that contains a NaiveBayesAnalyzer (an NLTK classifier trained on a large movie reviews corpus). \n",
      "\n",
      "Lastly, a dictionary with the media post, and its associated sentiment is constructed and saved into a json file for its posterior spatial analysis. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# function that analizes sentiment of a sentence, based on an LDA model, trained with the NLTK library\n",
      "def get_sentiment(text):\n",
      "    # create a textblob object with every tweet and return a tuple with its sentiment\n",
      "    blob = TextBlob(text, analyzer=NaiveBayesAnalyzer())\n",
      "    return blob.sentiment"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# for every tweet, do a sentiment analysis, and add that to a new json file\n",
      "for tweet in tweets: \n",
      "    tid, lat, lon, content = tweet\n",
      "    sentiment, pos, neg = get_sentiment(content)\n",
      "    with open('json_twitter/%ssentiment_tweet.json' %(tid), 'w' ) as f:\n",
      "        f.write( json.dumps({'lat':lat, 'lon':lon, 'tweet':content, 'sentiment':sentiment, 'pos':pos, 'neg':neg}))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Venue Type Analysis"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Foursquare\u2019s explore endpoint allows to search for up to 50 venues at a give geographical location, within a given radii. The venues should be queried by specific type. To overcome these limitations, NYC was divided into a grid of latitudes and longitudes, and queried sequentially per venue type. \n",
      "\n",
      "In order to quantify the types of venues existing in a neighborhood, the json files were open, assigned a venue type values, and finally saved into a format that can be easily spatially queried later. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from os import listdir\n",
      "from os.path import isfile, isdir, join\n",
      "from ttp import ttp\n",
      "\n",
      "import json\n",
      "import csv"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# function that opens twitter parsed jsons on a given file path, and adds the venue type to the dictionary\n",
      "# the sentence must contain words that are mostly english\n",
      "def add_venue_type(foursq_path, venue_type):\n",
      "    venue_path = join(foursq_path, venue_type)\n",
      "    \n",
      "    foursq_jsons = [ f for f in listdir(venue_path) if isfile(join(venue_path,f))]\n",
      "    \n",
      "    all_venues = []\n",
      "    for f in foursq_jsons:\n",
      "        with open(join(venue_path,f)) as json_file:\n",
      "            venues = json.load(json_file).values()\n",
      "            for venue in venues:\n",
      "                venue['venue_type'] = venue_type\n",
      "                all_venues.append(venue)\n",
      "    return all_venues"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "foursq_path = '../python/foursquare_explore/'\n",
      "venue_types = [direct for direct in listdir(foursq_path) if isdir(join(foursq_path, direct))]\n",
      "\n",
      "# create a single list of venue dictionaries with the venue type added\n",
      "all_venues = []\n",
      "for venue_type in venue_types:\n",
      "    all_venues.extend(add_venue_type(foursq_path, venue_type))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# write the list into a single json file\n",
      "with open('json_foursquare/all_venues.json' , 'w' ) as f:\n",
      "    f.write( json.dumps(all_venues))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Cloud Prcoessing"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "AWS was used to process the large dataset of social media posts. A mapping code was built and then uploaded to AWS with the boto library."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\"\n",
      "Phil's AWS Account\n",
      "AWSAccessKeyId= xxxxxxx\n",
      "AWSSecretKey= xxxxxxx\n",
      "\"\"\"\n",
      "\n",
      "# Load Credentials\n",
      "from boto.emr.connection import EmrConnection\n",
      "from boto.emr.step import StreamingStep\n",
      "import boto.emr"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "conn = EmrConnection('xxxxxxx', 'xxxxxxx')\n",
      "\n",
      "# Specifiy job name, mapper, input and output directory\n",
      "step = StreamingStep(name='topicmodel',\n",
      "                       mapper='s3://tweetclustering/mapper/topicmodel.py',\n",
      "                       reducer='aggregate',\n",
      "                       input='s3://tweetclustering/input',\n",
      "                       output='s3://tweetclustering/output')\n",
      "\n",
      "# Setup Job Flow\n",
      "impala_install_params = ['--install-impala', '--base-path', 's3://elasticmapreduce','--impala-version', 'latest']\n",
      "action = BootstrapAction(\"Custom\",\n",
      "                        \"s3://awsdocs/gettingstarted/latest/sentiment/config-nltk.sh\", impala_install_params)\n",
      "\n",
      "jobid = conn.run_jobflow(name='topicmodel',\n",
      "                         log_uri='s3://tweetclustering/jobflow_logs',\n",
      "                         steps=[step],  # step defined elsewhere\n",
      "                         bootstrap_actions=[action])\n",
      "\n",
      "\n",
      "# Get Status Report\n",
      "status = conn.describe_jobflow(jobid)\n",
      "status.state\n",
      "\n",
      "# Terminate Job\n",
      "conn.terminate_jobflow('topicmodel')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}