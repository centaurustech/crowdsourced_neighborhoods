{
 "metadata": {
  "name": "",
  "signature": "sha256:39eeb1dc55c612552b4be81e4f4b9da46d241b88113bea4cf31ddd86b494d036"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import logging\n",
      "\n",
      "from gensim import corpora, models, similarities\n",
      "from itertools import chain\n",
      "import nltk\n",
      "from nltk.corpus import stopwords\n",
      "from operator import itemgetter\n",
      "import re\n",
      "\n",
      "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class MyCorpus(object):\n",
      "    def __iter__(self):\n",
      "        for line in open('mycorpus.txt'):\n",
      "            # assume there's one document per line, tokens separated by whitespace\n",
      "            yield dictionary.doc2bow(line.lower().split())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#corpus_memory_friendly = MyCorpus() # doesn't load the corpus into memory!\n",
      "#print(corpus_memory_friendly)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "'''for vector in corpus_memory_friendly: # load one vector into memory at a time\n",
      "    print(vector)'''"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 5,
       "text": [
        "'for vector in corpus_memory_friendly: # load one vector into memory at a time\\n    print(vector)'"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#nltk.download(corpora/stopwords)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# collect statistics about all tokens\n",
      "dictionary = corpora.Dictionary(line.lower().split() for line in open('mycorpus.txt'))\n",
      "\n",
      "stoplist = stopwords.words('english')\n",
      "stop_ids = [dictionary.token2id[stopword] for stopword in stoplist if stopword in dictionary.token2id]\n",
      "once_ids = [tokenid for tokenid, docfreq in dictionary.dfs.iteritems() if docfreq == 1]\n",
      "# remove stop words and words that appear only once\n",
      "dictionary.filter_tokens(stop_ids + once_ids) \n",
      "# remove gaps in id sequence after words that were removed\n",
      "dictionary.compactify() \n",
      "print(dictionary)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Dictionary(12 unique tokens: [u'minors', u'graph', u'system', u'trees', u'eps']...)\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for i in dictionary: print i"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0\n",
        "1\n",
        "2\n",
        "3\n",
        "4\n",
        "5\n",
        "6\n",
        "7\n",
        "8\n",
        "9\n",
        "10\n",
        "11\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "'''dictionary.save('/tmp/deerwester.dict')\n",
      "corpus = [dictionary.doc2bow(text) for text in texts]\n",
      "corpora.MmCorpus.serialize('/tmp/deerwester.mm', corpus) # store to disk, for later use'''"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 8,
       "text": [
        "\"dictionary.save('/tmp/deerwester.dict')\\ncorpus = [dictionary.doc2bow(text) for text in texts]\\ncorpora.MmCorpus.serialize('/tmp/deerwester.mm', corpus) # store to disk, for later use\""
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import warnings\n",
      "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
      "from math import sqrt\n",
      "import gensim\n",
      "from sklearn.svm import SVC\n",
      "import os"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Load in corpus, remove newlines, make strings lower-case\n",
      "docs = {}\n",
      "corpus_dir = 'corpus'\n",
      "for filename in os.listdir(corpus_dir):\n",
      "    path = os.path.join(corpus_dir, filename)\n",
      "    doc = open(path).read().strip().lower()\n",
      "    docs[filename] = doc\n",
      "names = docs.keys()\n",
      "\n",
      "#Remove stopwords and split on spaces\n",
      "print \"n---Corpus with Stopwords Removed---\"\n",
      "stoplist = stopwords.words('english')\n",
      "preprocessed_docs = {}\n",
      "for name in names:\n",
      "    text = docs[name].split()\n",
      "    preprocessed = [word for word in text if word not in stoplist]\n",
      "    preprocessed_docs[name] = preprocessed\n",
      "    print name, \":\", preprocessed"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "n---Corpus with Stopwords Removed---\n",
        "sandwich2.txt : ['sandwich', 'meat', 'meat', 'cheese', 'meat']\n",
        "dog2.txt : ['dog', 'runs', 'barks', 'apatosaurus']\n",
        "dog1.txt : ['dog', 'runs', 'barks', 'dog']\n",
        "sandwich1.txt : ['\"a', 'sandwich', 'cheese', 'meat', 'bread', 'cheese', 'supercalifragilisticexpialidocious']\n"
       ]
      }
     ],
     "prompt_number": 24
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Build the dictionary and filter out common/rare terms\n",
      "dct = gensim.corpora.Dictionary(preprocessed_docs.values())\n",
      "unfiltered = dct.token2id.keys()\n",
      "dct.filter_extremes(no_below=2)\n",
      "filtered = dct.token2id.keys()\n",
      "filtered_out = set(unfiltered) - set(filtered)\n",
      "print \"nThe following super common/rare words were filtered out...\"\n",
      "print list(filtered_out), 'n'\n",
      "print \"Vocabulary after filtering...\"\n",
      "print dct.token2id.keys(), 'n'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "nThe following super common/rare words were filtered out...\n",
        "[u'apatosaurus', u'bread', u'\"a', u'supercalifragilisticexpialidocious'] n\n",
        "Vocabulary after filtering...\n",
        "[u'cheese', u'runs', u'sandwich', u'meat', u'barks', u'dog'] n\n"
       ]
      }
     ],
     "prompt_number": 26
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def vec2dense(vec, num_terms):\n",
      "\n",
      "    '''Convert from sparse gensim format to dense list of numbers'''\n",
      "    return list(gensim.matutils.corpus2dense([vec], num_terms=num_terms).T[0])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 30
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Build Bag of Words Vectors out of preprocessed corpus\n",
      "print \"---Bag of Words Corpus---\"\n",
      "\n",
      "bow_docs = {}\n",
      "for name in names:\n",
      "    sparse = dct.doc2bow(preprocessed_docs[name])\n",
      "    bow_docs[name] = sparse\n",
      "    \n",
      "    dense = vec2dense(sparse, num_terms=len(dct))\n",
      "    print name, \":\", dense"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "---Bag of Words Corpus---\n",
        "sandwich2.txt : [1.0, 0.0, 1.0, 3.0, 0.0, 0.0]\n",
        "dog2.txt : [0.0, 1.0, 0.0, 0.0, 1.0, 1.0]\n",
        "dog1.txt : [0.0, 1.0, 0.0, 0.0, 1.0, 2.0]\n",
        "sandwich1.txt : [2.0, 0.0, 1.0, 1.0, 0.0, 0.0]\n"
       ]
      }
     ],
     "prompt_number": 31
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Dimensionality reduction using LSI. Go from 6D to 2D.\n",
      "print \"n---LSI Model---\"\n",
      "lsi_docs = {}\n",
      "num_topics = 2\n",
      "lsi_model = gensim.models.LsiModel(bow_docs.values(),\n",
      "                                   num_topics=num_topics)\n",
      "for name in names:\n",
      "    vec = bow_docs[name]\n",
      "    sparse = lsi_model[vec]\n",
      "    dense = vec2dense(sparse, num_topics)\n",
      "    lsi_docs[name] = sparse\n",
      "    print name, ':', dense"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:gensim.models.lsimodel:no word id mapping provided; initializing from corpus, assuming identity\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "n---LSI Model---\n",
        "sandwich2.txt : [3.222517, 0.0]\n",
        "dog2.txt : [0.0, 1.6870012]\n",
        "dog1.txt : [0.0, 2.4343436]\n",
        "sandwich1.txt : [2.1483445, 0.0]\n"
       ]
      }
     ],
     "prompt_number": 32
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Normalize LSI vectors by setting each vector to unit length\n",
      "print \"\\n---Unit Vectorization---\"\n",
      "\n",
      "unit_vecs = {}\n",
      "for name in names:\n",
      "\n",
      "    vec = vec2dense(lsi_docs[name], num_topics)\n",
      "    norm = sqrt(sum(num ** 2 for num in vec))\n",
      "    unit_vec = [num / norm for num in vec]\n",
      "    unit_vecs[name] = unit_vec\n",
      "    print name, ':', unit_vec"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "---Unit Vectorization---\n",
        "sandwich2.txt : [1.0, 0.0]\n",
        "dog2.txt : [0.0, 1.0]\n",
        "dog1.txt : [0.0, 1.0]\n",
        "sandwich1.txt : [1.0, 0.0]\n"
       ]
      }
     ],
     "prompt_number": 33
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Take cosine distances between docs and show best matches\n",
      "print \"\\n---Document Similarities---\"\n",
      "\n",
      "index = gensim.similarities.MatrixSimilarity(lsi_docs.values())\n",
      "for i, name in enumerate(names):\n",
      "\n",
      "    vec = lsi_docs[name]\n",
      "    sims = index[vec]\n",
      "    sims = sorted(enumerate(sims), key=lambda item: -item[1])\n",
      "\n",
      "    #Similarities are a list of tuples of the form (doc #, score)\n",
      "    #In order to extract the doc # we take first value in the tuple\n",
      "    #Doc # is stored in tuple as numpy format, must cast to int\n",
      "\n",
      "    if int(sims[0][0]) != i:\n",
      "        match = int(sims[0][0])\n",
      "    else:\n",
      "        match = int(sims[1][0])\n",
      "\n",
      "    match = names[match]\n",
      "    print name, \"is most similar to...\", match"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:gensim.similarities.docsim:scanning corpus to determine the number of features (consider setting `num_features` explicitly)\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "---Document Similarities---\n",
        "sandwich2.txt is most similar to... sandwich1.txt\n",
        "dog2.txt is most similar to... dog1.txt\n",
        "dog1.txt is most similar to... dog2.txt\n",
        "sandwich1.txt is most similar to... sandwich2.txt\n"
       ]
      }
     ],
     "prompt_number": 35
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#We add classes to the mix by labelling dog1.txt and sandwich1.txt\n",
      "#We use these as our training set, and test on all documents.\n",
      "print \"\\n---Classification---\"\n",
      "\n",
      "dog1 = unit_vecs['dog1.txt']\n",
      "sandwich1 = unit_vecs['sandwich1.txt']\n",
      "\n",
      "train = [dog1, sandwich1]\n",
      "\n",
      "# The label '1' represents the 'dog' category\n",
      "# The label '2' represents the 'sandwich' category\n",
      "\n",
      "label_to_name = dict([(1, 'dogs'), (2, 'sandwiches')])\n",
      "labels = [1, 2]\n",
      "classifier = SVC()\n",
      "classifier.fit(train, labels)\n",
      "\n",
      "for name in names:\n",
      "\n",
      "    vec = unit_vecs[name]\n",
      "    label = classifier.predict([vec])[0]\n",
      "    cls = label_to_name[label]\n",
      "    print name, 'is a document about', cls\n",
      "\n",
      "print '\\n'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "---Classification---\n",
        "sandwich2.txt is a document about sandwiches\n",
        "dog2.txt is a document about dogs\n",
        "dog1.txt is a document about dogs\n",
        "sandwich1.txt is a document about sandwiches\n",
        "\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 38
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print unit_vecs"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "{'sandwich2.txt': [1.0, 0.0], 'dog2.txt': [0.0, 1.0], 'dog1.txt': [0.0, 1.0], 'sandwich1.txt': [1.0, 0.0]}\n"
       ]
      }
     ],
     "prompt_number": 39
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}