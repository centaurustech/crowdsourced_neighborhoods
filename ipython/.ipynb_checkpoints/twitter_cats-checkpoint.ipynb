{
 "metadata": {
  "name": "",
  "signature": "sha256:945028ad7ac8d83d46b453d17e872024bad79f46feab188684f05b4cd469487e"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from os import listdir\n",
      "from os.path import isfile, join\n",
      "from ttp import ttp\n",
      "from nltk.corpus import stopwords\n",
      "from gensim import corpora, models, similarities\n",
      "\n",
      "import json\n",
      "import csv\n",
      "\n",
      "def parse_jsons(twi_path):\n",
      "    twi_jsons = [ f for f in listdir(twi_path) if isfile(join(twi_path,f)) ][:100]\n",
      "\n",
      "    all_ids = []\n",
      "    for f in twi_jsons:\n",
      "        with open('%s%s' %(twi_path, f)) as json_file:\n",
      "            tweets = json.load(json_file).values()\n",
      "            for tweet in tweets:\n",
      "                content = tweet['content']\n",
      "                p = ttp.Parser()\n",
      "                #parsed_tweet = p.parse(content)\n",
      "                #hashtags = [tag.encode('utf-8') for tag in parsed_tweet.tags]\n",
      "                if float(tweet['lat']) != 0 and float(tweet['lon']) != 0:\n",
      "                    if tweet['tweet_id'] not in all_ids:\n",
      "                        all_ids.append(tweet['tweet_id'])\n",
      "                        yield tweet['content']#.encode('utf-8')\n",
      "    \n",
      "def make_texts(tweets):\n",
      "    stoplist = stopwords.words('english')\n",
      "    for tweet in tweets:\n",
      "        yield [word for word in tweet.lower().split() if word not in stoplist]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 330
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "twi_path = '../python/twitter/'\n",
      "tweets = parse_jsons(twi_path)\n",
      "texts = make_texts(tweets)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 331
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "all_tokens = sum(texts, [])\n",
      "tokens_once = set(word for word in set(all_tokens) if all_tokens.count(word) == 1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 332
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "texts = [[word for word in text if word not in tokens_once] for text in make_texts(parse_jsons(twi_path))]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 333
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Create Dictionary.\n",
      "id2word = corpora.Dictionary(make_texts(parse_jsons(twi_path)))\n",
      "\n",
      "# TOKENIZERATOR: collect statistics about all tokens\n",
      "dictionary = corpora.Dictionary(make_texts(parse_jsons(twi_path)))\n",
      "mm = [id2word.doc2bow(text) for text in make_texts(parse_jsons(twi_path))]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 334
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Trains the LDA models.\n",
      "lda = models.ldamodel.LdaModel(corpus=mm, id2word=id2word, num_topics=20, update_every=1, chunksize=10000, passes=1)\n",
      "\n",
      "print lda"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "LdaModel(num_terms=9309, num_topics=20, decay=0.5, chunksize=10000, alpha=[ 0.05  0.05  0.05  0.05  0.05  0.05  0.05  0.05  0.05  0.05  0.05  0.05\n",
        "  0.05  0.05  0.05  0.05  0.05  0.05  0.05  0.05])\n"
       ]
      }
     ],
     "prompt_number": 335
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Prints the topics.\n",
      "for top in lda.print_topics():\n",
      "  print top\n",
      "print"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.007*get + 0.006*@ + 0.005*i'm + 0.005*can't + 0.005*like + 0.004*\u2716\ufe0f\u2716\ufe0f\u2716\ufe0f\u2716\ufe0f + 0.004*wait + 0.003*u + 0.003*new + 0.003*next\n",
        "0.006*@ + 0.005*that's + 0.005*u + 0.004*i'm + 0.004*wanna + 0.004*follow + 0.004*thanks + 0.003*- + 0.003*new + 0.003*love\n",
        "0.018*@ + 0.006*lol + 0.006*i'm + 0.005*#nyc + 0.005*&amp; + 0.005*got + 0.004*new + 0.003*- + 0.003*first + 0.003*night\n",
        "0.013*@ + 0.007*can't + 0.005*i'm + 0.004*de + 0.004*happy + 0.003*getting + 0.003*us + 0.003*it's + 0.003*brooklyn + 0.003*work\n",
        "0.009*new + 0.007*ny + 0.006*@ + 0.006*i'm + 0.006*york, + 0.004*hip + 0.004*love + 0.004*girls + 0.004*bae + 0.004*\ud83d\ude0d\n",
        "0.007*@ + 0.005*love + 0.005*new + 0.005*i'm + 0.005*don't + 0.004*ko + 0.004*day + 0.004*show + 0.003*ny) + 0.003*&amp;\n",
        "0.013*@ + 0.008*new + 0.005*york, + 0.005*- + 0.005*ny) + 0.005*&amp; + 0.005*it's + 0.005*i'm + 0.005*time + 0.005*(@\n",
        "0.026*@ + 0.008*don't + 0.008*i'm + 0.006*- + 0.006*ny + 0.005*love + 0.004*new + 0.004*center + 0.003*park + 0.003*u\n",
        "0.010*@ + 0.007*i'm + 0.006*&amp; + 0.005*- + 0.005*don't + 0.004*love + 0.003*gonna + 0.003*one + 0.003*get + 0.002*ny\n",
        "0.010*@ + 0.008*i'm + 0.008*new + 0.005*i've + 0.005*#somethingbigvideo + 0.004*think + 0.004*don't + 0.004*go + 0.004*still + 0.004*back\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 336
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "'''\n",
      "# TOKENIZERATOR: collect statistics about all tokens\n",
      "dictionary = corpora.Dictionary(line.lower().split() for line in tweets)\n",
      "\n",
      "#print (dictionary)\n",
      "#print (dictionary.token2id)'''"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 337,
       "text": [
        "'\\n# TOKENIZERATOR: collect statistics about all tokens\\ndictionary = corpora.Dictionary(line.lower().split() for line in tweets)\\n\\n#print (dictionary)\\n#print (dictionary.token2id)'"
       ]
      }
     ],
     "prompt_number": 337
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "'''stoplist = stopwords.words('english')\n",
      "\n",
      "# DICTERATOR: remove stop words and words that appear only once \n",
      "stop_ids = [dictionary.token2id[stopword] for stopword in stoplist if stopword in dictionary.token2id]\n",
      "once_ids = [tokenid for tokenid, docfreq in dictionary.dfs.iteritems() if docfreq == 1]\n",
      "dictionary.filter_tokens(stop_ids + once_ids)\n",
      "print (dictionary)'''"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 338,
       "text": [
        "\"stoplist = stopwords.words('english')\\n\\n# DICTERATOR: remove stop words and words that appear only once \\nstop_ids = [dictionary.token2id[stopword] for stopword in stoplist if stopword in dictionary.token2id]\\nonce_ids = [tokenid for tokenid, docfreq in dictionary.dfs.iteritems() if docfreq == 1]\\ndictionary.filter_tokens(stop_ids + once_ids)\\nprint (dictionary)\""
       ]
      }
     ],
     "prompt_number": 338
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "'''dictionary.compactify() # remove gaps in id sequence after words that were removed\n",
      "print dictionary'''"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 339,
       "text": [
        "'dictionary.compactify() # remove gaps in id sequence after words that were removed\\nprint dictionary'"
       ]
      }
     ],
     "prompt_number": 339
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "'''class MyCorpus(object):\n",
      "    def __iter__(self):\n",
      "        for line in open('mycorpus.txt'):\n",
      "            # assume there's one document per line, tokens separated by whitespace\n",
      "            yield dictionary.doc2bow(line.lower().split()) '''"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 340,
       "text": [
        "\"class MyCorpus(object):\\n    def __iter__(self):\\n        for line in open('mycorpus.txt'):\\n            # assume there's one document per line, tokens separated by whitespace\\n            yield dictionary.doc2bow(line.lower().split()) \""
       ]
      }
     ],
     "prompt_number": 340
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "'''# VECTORERATOR: map tokens frequency per doc to vectors\n",
      "corpus_memory_friendly = MyCorpus() # doesn't load the corpus into memory!\n",
      "for item in corpus_memory_friendly:\n",
      "    print item'''"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 341,
       "text": [
        "\"# VECTORERATOR: map tokens frequency per doc to vectors\\ncorpus_memory_friendly = MyCorpus() # doesn't load the corpus into memory!\\nfor item in corpus_memory_friendly:\\n    print item\""
       ]
      }
     ],
     "prompt_number": 341
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 341
    }
   ],
   "metadata": {}
  }
 ]
}