{
 "metadata": {
  "name": "",
  "signature": "sha256:51d6f2e28f3d231d4822d1673c69bc9521b163fdf847ffded087dc24a9e6df96"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Data Scrapping"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Central to differentiating our process from existing examples of neighborhood ranking systems was the implementation interpretations based on social media and more dynamic datasets. \n",
      "\n",
      "The following set of functions were divided in a number of individual, executable python scripts that were run for over three weeks on a server/vm at MIT. The scripts were used to scrape data from social media and open web APIs. The scripts also parsed the responses obtained from the APIs into readable dictionaries that made it easier to obtain the desired information for the project. \n",
      "\n",
      "Finally, the dictionaries were stored on the server, and later uploaded to github for file management. "
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Code for scrapping trading venues on Foursquare"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The following code hits foursquare's trending endpoint every 20 seconds for very long period of time. Following the response from the server, the data is parsed into a useful format and then saved into a json dictionary. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os\n",
      "import json\n",
      "import time\n",
      "import sys\n",
      "from datetime import datetime\n",
      "import foursquare"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Set the API keys\n",
      "client_id = 'W5UZGTZGO2TJELFWQF1IPYWJ2UXX1WEY2FFZBS14QLXOBKS1'\n",
      "client_secret = 'C2RXMQINBN3ZAOBX3QIOBTYVGXDGWYTPRO5GKNWL0AOC4T12'\n",
      "redirect = 'http://example.com'\n",
      "\n",
      "# set the total time we will be hitting the API per individual json\n",
      "total_time = 160 "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_checkins(ll):\n",
      "    # Construct the client object\n",
      "    client = foursquare.Foursquare(client_id=client_id, client_secret=client_secret, redirect_uri=redirect)\n",
      "    # get the response from the API\n",
      "    response = client.venues.trending(params={'near': ll, 'limit':'1000', 'radius': '7500'})\n",
      "    return response\n",
      "\n",
      "def get_many_checkins(lls, total_time, names):\n",
      "    all_checkins = {}\n",
      "    remaining_seconds = total_time\n",
      "    interval = 20\n",
      "\n",
      "    while remaining_seconds > 0:\n",
      "        added = 0\n",
      "        for nid, ll in enumerate(lls):\n",
      "            print 'scrapping: ', names[nid]\n",
      "            t = get_checkins(ll)\n",
      "            #print t\n",
      "            new_checkins = t['venues']\n",
      "            for checkin in new_checkins:\n",
      "                check_id = checkin['id']\n",
      "                if check_id not in all_checkins:\n",
      "                    properties = {}\n",
      "                    \n",
      "                    properties['content'] = checkin['name']\n",
      "                    properties['here_now'] = checkin['hereNow']['count']\n",
      "                    properties['checkins'] = checkin['stats']['checkinsCount']\n",
      "                    properties['category'] = checkin['categories'][0]['name']\n",
      "                    properties['lat'] = checkin['location']['lat']\n",
      "                    properties['lon'] = checkin['location']['lng']\n",
      "                    properties['checkins'] = checkin['stats']['checkinsCount']\n",
      "                    properties['users_count'] = checkin['stats']['usersCount']\n",
      "                    properties['place_id'] = check_id\n",
      "                    properties['data_point'] = 'none'\n",
      "                    properties['raw_source'] = checkin\n",
      "                    properties['time'] = str(datetime.now())\n",
      "                    properties['data_source'] = 'foursquare_trending'\n",
      "                    \n",
      "                    all_checkins[check_id] = properties\n",
      "                    added += 1\n",
      "        print \"At %d seconds, added %d new checkins, for a total of %d\" % ( total_time - remaining_seconds, added, len(all_checkins))\n",
      "        time.sleep(interval)\n",
      "        remaining_seconds -= interval\n",
      "    return all_checkins\n",
      "    \n",
      "def run(lls, names):\n",
      "    checkins = get_many_checkins(lls, total_time, names)\n",
      "    name =  str(datetime.now()).replace(\" \", \"_\")\n",
      "    name = name.replace('.', '_')\n",
      "    name = name.replace(':', '_')\n",
      "    \n",
      "    #upload = upload_to_s3( target_path, json.dumps(checkins))\n",
      "    with open( 'foursquare/%sfour_trending.json' %(name), 'w' ) as f:\n",
      "        f.write(json.dumps(checkins))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# locations for five boroughs \n",
      "lls = ['40.7127, -74.0059', '40.634525, -73.945806', '40.608628, -74.086612', '40.703947, -73.84949', '40.830848, -73.916423']\n",
      "# names of the 5 buroughs to add to the file name\n",
      "names = ['NY', 'Brooklyn', 'Staten Island', 'Queens', 'Bronx']\n",
      "\n",
      "all_time = 9000000000000000000\n",
      "while all_time > 0:\n",
      "    try:\n",
      "        run(lls, names)\n",
      "        all_time -= 30\n",
      "    except: pass"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "''' \n",
      "this scrapes data from foursquare\n",
      "'''"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 6,
       "text": [
        "' \\nthis scrapes data from foursquare\\n'"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "client_id = 'W5UZGTZGO2TJELFWQF1IPYWJ2UXX1WEY2FFZBS14QLXOBKS1'#os.environ['FOURSQUARE_ID']\n",
      "client_secret = 'C2RXMQINBN3ZAOBX3QIOBTYVGXDGWYTPRO5GKNWL0AOC4T12'#os.environ['FOURSQUARE_SECRET']\n",
      "redirect = 'http://sg20141.sb02.stations.graphenedb.com:24789/browser/'#os.environ['FOURSQUARE_REDIRECT']\n",
      "\n",
      "query = 'topPicks'\n",
      "queries = ['food', 'drinks', 'coffee', 'shops', 'arts', 'outdoors', 'sights', 'specials', 'topPicks']\n",
      "#queries = ['arts', 'sights', 'specials', 'topPicks']\n",
      "\n",
      "\n",
      "# Construct the client object\n",
      "client = foursquare.Foursquare(client_id=client_id, client_secret=client_secret, redirect_uri=redirect)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def drange(start, stop, step):\n",
      "    r = start\n",
      "    while r < stop:\n",
      "        yield r\n",
      "        r += step\n",
      "\n",
      "def get_checkins(ll, query):\n",
      "    # Assign a search radius\n",
      "    rad = '150'\n",
      "    # get the response from the API\n",
      "    response = client.venues.explore(params={'ll': ll, 'radius': rad, 'section': query, 'limit': '1000000'})\n",
      "    return response\n",
      "\n",
      "def get_venue(check_id):\n",
      "    # Get client response\n",
      "    r = client.venues(check_id)\n",
      "    #print r\n",
      "    # Construct an empty dictionary for the properties\n",
      "    properties = {}\n",
      "    # Get the information of the venue\n",
      "    venue = r['venue']\n",
      "    # Add keys to the dictionary\n",
      "    properties['place_id'] = check_id\n",
      "    properties['name'] = venue['name']\n",
      "    properties['user'] = [item['user']['id'] for item in venue['tips']['groups'][0]['items']]\n",
      "    properties['checkins'] = venue['stats']['checkinsCount']\n",
      "    properties['users_count'] = venue['stats']['usersCount']\n",
      "    properties['category'] = venue['categories'][0]['name']\n",
      "    properties['latitude'] = venue['location']['lat']\n",
      "    properties['longitude'] = venue['location']['lng']\n",
      "    try: properties['rating'] = venue['rating']\n",
      "    except: properties['rating'] =  ''\n",
      "    try: properties['hours'] = venue['hours']['timeframes']\n",
      "    except: properties['hours'] = ''\n",
      "    try: properties['twitter'] = venue['contact']['twitter']\n",
      "    except: properties['twitter'] = ''\n",
      "    try: properties['popular'] = venue['popular']['timeframes']\n",
      "    except: properties['popular'] = ''\n",
      "    properties['time'] = str(datetime.now())\n",
      "    properties['data_source'] = 'foursquare_explore'\n",
      "    \n",
      "    # return the dictionary of properties\n",
      "    return properties\n",
      "\n",
      "def crawl_4square(query):\n",
      "    # since the api only returns up to 50 places at the time,\n",
      "    # we construct a series of locations, and loop through them to scrape it\n",
      "    we_range = list(drange(-74.290503, -73.702553, .003)) # 50.5 mts radius\n",
      "    sn_range = list(drange(40.482003, 40.918004, .003)) # 50.5 mts radius # 113.8352\n",
      "    print len(sn_range), len(we_range)\n",
      "    # for every latitude\n",
      "    for n in sn_range[134:]:\n",
      "        # for every longitude\n",
      "        for e in we_range[30:150]:\n",
      "            # construct a location\n",
      "            ll = str(n) + ', ' + str(e)\n",
      "            print ll\n",
      "            \n",
      "            try:\n",
      "                # Get all the checkins for a given location\n",
      "                t = get_checkins(ll, query)\n",
      "                new_checkins = t['groups'][0]['items']\n",
      "                all_checkins = {}\n",
      "                # for every checkin in the checkinsggggg\n",
      "                for checkin in new_checkins:\n",
      "                    # Get the venue ID\n",
      "                    check_id = checkin['venue']['id']\n",
      "                    # if the id is not in the dictionary, add it\n",
      "                    if check_id not in all_checkins:\n",
      "                        all_checkins[check_id] = get_venue(check_id)\n",
      "                #print len(all_checkins)\n",
      "                # write the json to a file\n",
      "                if len(all_checkins) > 0:\n",
      "                    print 'places added: ', len(all_checkins)\n",
      "                    with open( 'foursquare_explore/%s/%s,%sfour_explore.json' %(query, str(sn_range.index(n)), str(we_range.index(e))), 'w' ) as f:\n",
      "                        f.write(json.dumps(all_checkins))\n",
      "            except: pass"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sw = '22.1538, 113.8352'\n",
      "ne = '22.5622, 114.4416'\n",
      "sw = '-74.290503, 40.482003'\n",
      "ne = '-73.702553, 40.918004'\n",
      "\n",
      "\n",
      "if __name__=='__main__':\n",
      "    # Construct a dictionary for all the checkins\n",
      "    for query in queries:\n",
      "        querying = crawl_4square(query)\n",
      "   "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "''' \n",
      "this scrapes data from twitter\n",
      "'''"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from datetime import datetime\n",
      "from twython import Twython\n",
      "\n",
      "APP_KEY = '5g8MCsu7a2e74JRORQ22G76uy'\n",
      "APP_SECRET = 'qcwgzUGzgELMFRHQI1tZqsZtvkTTbQxp7KUnjQnR3WjKMin0Ff'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_tweets( latlong=None ):\n",
      "    ''' Fetches tweets with a given query at a given lat-long.'''\n",
      "    twitter = Twython( APP_KEY, APP_SECRET )\n",
      "    results = twitter.search( geocode=','.join([ str(x) for x in latlong ]) + ',15km', result_type='recent', count=100 )\n",
      "    return results['statuses']\n",
      "\n",
      "\n",
      "def get_lots_of_tweets( latlongs, names):\n",
      "    \"\"\" Does pretty much what its long name suggests. \"\"\"\n",
      "    all_tweets = {}\n",
      "    total_time = 160\n",
      "    remaining_seconds = total_time\n",
      "    interval = 20 \n",
      "    while remaining_seconds > 0:\n",
      "        added = 0\n",
      "        new_tweets = []\n",
      "        for nid, latlong in enumerate(latlongs):\n",
      "            print 'scrapping: ', names[nid]\n",
      "            new_tweets.extend(get_tweets( latlong ))\n",
      "        for tweet in new_tweets:\n",
      "            tid = tweet['id']\n",
      "            if tid not in all_tweets and tweet['coordinates'] != None:\n",
      "                properties = {}\n",
      "                properties['lat'] = tweet['coordinates']['coordinates'][0]\n",
      "                properties['lon'] = tweet['coordinates']['coordinates'][1]\n",
      "                properties['tweet_id'] = tid\n",
      "                properties['content'] = tweet['text']\n",
      "                properties['user'] = tweet['user']['id']\n",
      "                properties['user_location'] = tweet['user']['location']\n",
      "                properties['raw_source'] = tweet\n",
      "                properties['data_point'] = 'none'\n",
      "                properties['time'] = tweet['created_at']\n",
      "                all_tweets[ tid ] = properties\n",
      "                added += 1\n",
      "        print \"At %d seconds, added %d new tweets, for a total of %d\" % (total_time - remaining_seconds, added, len( all_tweets ) )\n",
      "        time.sleep(interval)\n",
      "        remaining_seconds -= interval\n",
      "    return all_tweets\n",
      "\n",
      "\n",
      "def run(locations, names):\n",
      "    #for nid, location in enumerate(locations):\n",
      "    t = get_lots_of_tweets(locations, names)\n",
      "    name =  str(datetime.now()).replace(\" \", \"_\")\n",
      "    name = name.replace('.', '_')\n",
      "    name = name.replace(':', '_')\n",
      "    with open('twitter/%stweets.json' %(name), 'w' ) as f:\n",
      "        f.write( json.dumps(t))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "locations = [[40.7127, -74.0059], [40.634525, -73.945806], [40.608628, -74.086612], [40.703947, -73.84949], [40.830848, -73.916423]]\n",
      "names = ['NY', 'Brooklyn', 'Staten Island', 'Queens', 'Bronx']\n",
      "all_time = 9000000000000000000\n",
      "while all_time > 0:\n",
      "    try:\n",
      "        run(locations, names)\n",
      "        all_time -= 30\n",
      "    except: pass"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "''' \n",
      "this scrapes data from instagram\n",
      "'''"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 9,
       "text": [
        "' \\nthis scrapes data from instagram\\n'"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import requests\n",
      "import threading\n",
      "\n",
      "\n",
      "## You need to change this to your own info\n",
      "Client_ID = '9056a11deaef47a59abc8647208b58d7'\n",
      "Redirect_URI = 'http://www.nytimes.com'\n",
      "code='0340b48b153c4a4b99fe0f8b328a717a'\n",
      "access_token = '23252351.9056a11.93d34bc129f443cba0adef6de1532f8f'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def getfeed():\n",
      "    threading.Timer(120.0, getfeed).start()\n",
      "\n",
      "    t = time.strftime(\"%Y%m%d%H%M\")\n",
      "    # print t\n",
      "    with open('ID') as f:\n",
      "        ID = f.read().splitlines()\n",
      "    data = [None]*len(ID)\n",
      "    counter = 0\n",
      "    for item in ID[0:15]:\n",
      "        url = 'https://api.instagram.com/v1/locations/'+str(item)+'/media/recent?access_token='+access_token\n",
      "        # print url\n",
      "        r = requests.get(str(url))\n",
      "        data[counter] = r.json()\n",
      "\n",
      "        counter += 1\n",
      "    with open('instagram/feed'+t+'.json','a') as f: \n",
      "        json.dump(data, f)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "try: getfeed()\n",
      "except: pass"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "'''\n",
      "this parses the instagram feed\n",
      "'''"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 12,
       "text": [
        "'\\nthis parses the instagram feed\\n'"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from os import listdir\n",
      "from os.path import isfile, join\n",
      "from ttp import ttp\n",
      "import csv"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "insta_path = 'instagram/'\n",
      "insta_jsons = [ f for f in listdir(insta_path) if isfile(join(insta_path,f)) ]\n",
      "csv_file = open('csv/instagram_csv.csv', 'wt')\n",
      "\n",
      "writer = csv.writer(csv_file)\n",
      "writer.writerow(['pid', 'lat', 'lon', 'time', 'content', 'tags'])\n",
      "\n",
      "\n",
      "all_ids = []\n",
      "for f in insta_jsons:\n",
      "    with open('instagram/%s' %(f)) as json_file:\n",
      "        grams = json.load(json_file)\n",
      "        cnt = 0\n",
      "        for gram in grams:\n",
      "            if gram != None:\n",
      "                if len(gram['data']) != 0:\n",
      "                    for data in gram['data']:\n",
      "                        #image, user, content, time, iid, lat, lon = '', '', '', '', '', '', ''\n",
      "                        content, image, user, caption, time, iid, lat, lon = '', None, None, None, None, None, None, None\n",
      "                        try: image = data['images']['standard_resolution']['url']\n",
      "                        except: pass\n",
      "                        try: user = data['user']['id']\n",
      "                        except: pass\n",
      "                        try: content = data['caption']['text']\n",
      "                        except: pass\n",
      "                        try: time = data['caption']['created_time']\n",
      "                        except: pass\n",
      "                        try: iid = data['id']\n",
      "                        except: pass\n",
      "                        try: lat = data['location']['latitude']\n",
      "                        except: pass\n",
      "                        try: lon = data['location']['longitude']\n",
      "                        except: pass\n",
      "                        raw_source = data\n",
      "                        \n",
      "                        if lat != None and lon != None:\n",
      "                            if iid not in all_ids:\n",
      "                                p = ttp.Parser()\n",
      "                                hashtags = None\n",
      "                                parsed_gram = p.parse(content)\n",
      "                                hashtags = [tag.encode('utf-8') for tag in parsed_gram.tags]\n",
      "                                #print hashtags\n",
      "                                writer.writerow([iid, lat, lon, time, content.encode('utf-8'), str(hashtags)])\n",
      "                                \n",
      "                                all_ids.append(iid)\n",
      "                                cnt += 1                        \n",
      "                        \n",
      "    print \"Added %d new grams, for a total of %d\" % (cnt, len(all_ids))                        \n",
      "\n",
      "csv_file.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}