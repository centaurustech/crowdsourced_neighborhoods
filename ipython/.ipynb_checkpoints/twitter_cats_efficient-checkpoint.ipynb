{
 "metadata": {
  "name": "",
  "signature": "sha256:36bf35765de0be4349ab5f5e22573da9362426a8f0ae0064eb6e9f79bbed0414"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from os import listdir\n",
      "from os.path import isfile, join\n",
      "from ttp import ttp\n",
      "from nltk.corpus import stopwords\n",
      "from gensim import corpora, models, similarities\n",
      "from itertools import chain\n",
      "import numpy as np\n",
      "\n",
      "import json\n",
      "import csv"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 213
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "UPPERLETTERS = 'ABCDEFGHIJKLMNOPQRSTUVWXYZ'\n",
      "LETTERS_AND_SPACE = UPPERLETTERS + UPPERLETTERS.lower() + ' \\t\\n'\n",
      "\n",
      "def loadDictionary():\n",
      "    dictionaryFile = open('dictionary.txt')\n",
      "    englishWords = {}\n",
      "    for word in dictionaryFile.read().split('\\n'):\n",
      "        englishWords[word] = None\n",
      "    dictionaryFile.close()\n",
      "    return englishWords\n",
      "\n",
      "ENGLISH_WORDS = loadDictionary()\n",
      "\n",
      "def getEnglishCount(message):\n",
      "    message = message.upper()\n",
      "    message = removeNonLetters(message)\n",
      "    possibleWords = message.split()\n",
      "\n",
      "    if possibleWords == []:\n",
      "        # no words at all, so return 0.0\n",
      "        return 0.0, ''  \n",
      "\n",
      "    matches = 0\n",
      "    all_words = []\n",
      "    for word in possibleWords:\n",
      "        if word in ENGLISH_WORDS:\n",
      "            matches += 1\n",
      "            all_words.append(word + ' ')\n",
      "    return float(matches) / len(possibleWords), ''.join(all_words).lower()\n",
      "\n",
      "\n",
      "def removeNonLetters(message):\n",
      "    lettersOnly = []\n",
      "    for symbol in message:\n",
      "        if symbol in LETTERS_AND_SPACE:\n",
      "            lettersOnly.append(symbol)\n",
      "    return ''.join(lettersOnly)\n",
      "\n",
      "\n",
      "def isEnglish(message, wordPercentage=20, letterPercentage=85):\n",
      "    # By default, 20% of the words must exist in the dictionary file, and\n",
      "    # 85% of all the characters in the message must be letters or spaces\n",
      "    # (not punctuation or numbers).\n",
      "    cnt, sentence = getEnglishCount(message)\n",
      "    words_match = cnt * 100 >= wordPercentage\n",
      "    numLetters = len(removeNonLetters(message))\n",
      "    messageLettersPercentage = float(numLetters) / len(message) * 100\n",
      "    letters_match = messageLettersPercentage >= letterPercentage\n",
      "    return words_match and letters_match, sentence"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 214
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_tweet(twi_path):\n",
      "    twi_jsons = [ f for f in listdir(twi_path) if isfile(join(twi_path,f)) ][:100]\n",
      "\n",
      "    all_ids = []\n",
      "    for f in twi_jsons:\n",
      "        with open('%s%s' %(twi_path, f)) as json_file:\n",
      "            tweets = json.load(json_file).values()\n",
      "            for tweet in tweets:\n",
      "                content = tweet['content']\n",
      "\n",
      "                if float(tweet['lat']) != 0 and float(tweet['lon']) != 0:\n",
      "                    if tweet['tweet_id'] not in all_ids:\n",
      "                        all_ids.append(tweet['tweet_id'])\n",
      "                        yield tweet['content'].encode('utf-8')\n",
      "                        \n",
      "def jsons_to_tuple(twi_path):\n",
      "    twi_jsons = [ f for f in listdir(twi_path) if isfile(join(twi_path,f)) ][:100]\n",
      "\n",
      "    all_ids = []\n",
      "    for f in twi_jsons:\n",
      "        with open('%s%s' %(twi_path, f)) as json_file:\n",
      "            tweets = json.load(json_file).values()\n",
      "            for tweet in tweets:\n",
      "                content = tweet['content']\n",
      "                \n",
      "                is_english, new_content = isEnglish(content, 20, 65)\n",
      "                if is_english:\n",
      "                    if float(tweet['lat']) != 0 and float(tweet['lon']) != 0:\n",
      "                        if tweet['tweet_id'] not in all_ids:\n",
      "                            all_ids.append(tweet['tweet_id'])\n",
      "                            yield tweet['tweet_id'], tweet['lat'], tweet['lon'], new_content\n",
      "                        \n",
      "new_yorks = ['nyc', 'newyork', 'manhattan', 'brooklin', 'new_york', 'new york', 'brooklyn', 'ny', 'newyorkcity']\n",
      "\n",
      "def parse_jsons(twi_path):\n",
      "    twi_jsons = [ f for f in listdir(twi_path) if isfile(join(twi_path,f)) ][:100]\n",
      "\n",
      "    all_ids = []\n",
      "    for f in twi_jsons:\n",
      "        with open('%s%s' %(twi_path, f)) as json_file:\n",
      "            tweets = json.load(json_file).values()\n",
      "            for tweet in tweets:\n",
      "                content = tweet['content']\n",
      "                p = ttp.Parser()\n",
      "                parsed_tweet = p.parse(content)\n",
      "                hashtags = [tag.lower().encode('utf-8') for tag in parsed_tweet.tags if tag.lower().encode('utf-8') not in new_yorks]\n",
      "                if len(hashtags) > 0:\n",
      "                    if float(tweet['lat']) != 0 and float(tweet['lon']) != 0:\n",
      "                        if tweet['tweet_id'] not in all_ids:\n",
      "                            all_ids.append(tweet['tweet_id'])\n",
      "                            yield hashtags#tweet['content']#.encode('utf-8')\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 215
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "twi_path = '../python/twitter/'\n",
      "tweets = parse_jsons(twi_path)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 216
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# TOKENIZERATOR: collect statistics about all tokens\n",
      "dictionary = corpora.Dictionary(tweets)\n",
      "\n",
      "print (dictionary)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Dictionary(840 unique tokens: [u'sustainabledesign', u'vitamins', u'japan', u'fitfam', u'blackandwhite']...)\n"
       ]
      }
     ],
     "prompt_number": 217
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "stoplist = stopwords.words('english')\n",
      "\n",
      "# DICTERATOR: remove stop words and words that appear only once \n",
      "stop_ids = [dictionary.token2id[stopword] for stopword in stoplist if stopword in dictionary.token2id]\n",
      "once_ids = [tokenid for tokenid, docfreq in dictionary.dfs.iteritems() if docfreq == 1]\n",
      "dictionary.filter_tokens(stop_ids + once_ids)\n",
      "print (dictionary)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Dictionary(93 unique tokens: [u'datenight', u'dance', u'photo', u'goodtimes', u'broadway']...)\n"
       ]
      }
     ],
     "prompt_number": 218
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dictionary.compactify() # remove gaps in id sequence after words that were removed\n",
      "print dictionary"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Dictionary(93 unique tokens: [u'datenight', u'dance', u'photo', u'goodtimes', u'broadway']...)\n"
       ]
      }
     ],
     "prompt_number": 219
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_singles(dictionary, ids):\n",
      "    for word_id in once_ids:\n",
      "        yield dictionary.get(word_id)\n",
      "\n",
      "def filter_singles(singles, texts):\n",
      "    for text in texts:\n",
      "        new_list = []\n",
      "        for word in text:\n",
      "            if word not in singles:\n",
      "                new_list.append(word)\n",
      "        yield new_list\n",
      "        \n",
      "def mm_iterator(dictionary, texts):\n",
      "    for text in texts:\n",
      "        yield dictionary.doc2bow(text)\n",
      "                "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 220
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "singles = get_singles(dictionary, once_ids)\n",
      "filtered_texts = filter_singles(singles, parse_jsons(twi_path))\n",
      "# Create Bag of words\n",
      "# atlernatively for text in parse_jsons(twi_path)\n",
      "mm = [dictionary.doc2bow(text) for text in filtered_texts] #mm_iterator(dictionary, filtered_texts)\n",
      "#other_mm = mm_iterator(dictionary, parse_jsons(twi_path))#[dictionary.doc2bow(text) for text in make_texts(parse_jsons(twi_path))]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 221
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "''' the corpus can be obtained from a saved file somewhere instead of memory'''\n",
      "num_topics = 20\n",
      "\n",
      "# Trains the LDA models.\n",
      "lda = models.ldamodel.LdaModel(corpus=list(mm), id2word=dictionary, num_topics=num_topics, \n",
      "                               update_every=1, chunksize=10000, passes=10, iterations=50)\n",
      "\n",
      "for i in lda.print_topics(): print i"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.262*alavslsu + 0.176*smh + 0.090*handbags + 0.090*style + 0.004*centralpark + 0.004*latergram + 0.004*theview + 0.004*instagram + 0.004*competition + 0.004*design\n",
        "0.123*fashion + 0.123*style + 0.123*travel + 0.123*nov8 + 0.123*competition + 0.123*sheertalent + 0.003*fall + 0.003*centralpark + 0.003*hopkinskovalev + 0.003*downtown\n",
        "0.245*yamecanse + 0.205*yamecansedemurillokaram + 0.205*unionsquare + 0.083*coffee + 0.083*ebola + 0.002*theview + 0.002*instagram + 0.002*competition + 0.002*design + 0.002*votefifthharmony\n",
        "0.192*happyveteransday + 0.192*nycomedyfestival + 0.099*downtown + 0.099*williamsburg + 0.005*nycf + 0.005*dance + 0.005*selfie + 0.005*centralpark + 0.005*latergram + 0.005*foodporn\n",
        "0.173*broadway + 0.116*funfearlesslife + 0.116*marketing + 0.116*carlosbirthday + 0.116*newyorkdiaries + 0.059*dance + 0.059*lunch + 0.003*party + 0.003*hopkinskovalev + 0.003*totaltoptuesday\n",
        "0.308*photo + 0.008*dance + 0.008*centralpark + 0.008*selfie + 0.008*latergram + 0.008*foodporn + 0.008*coffee + 0.008*veteransday + 0.008*vote5sos + 0.008*theview\n",
        "0.116*festivaloflights + 0.116*beermenus + 0.116*goodtimes + 0.116*november + 0.116*onenewark + 0.059*beautiful + 0.059*theview + 0.059*dumbo + 0.003*totaltoptuesday + 0.003*hopkinskovalev\n",
        "0.150*instagram + 0.150*makeup + 0.150*empiresummit + 0.077*photography + 0.077*fashion + 0.077*job + 0.004*coffee + 0.004*fkatwigs + 0.004*centralpark + 0.004*timessquare\n",
        "0.164*music + 0.164*canifffollowme + 0.110*sushi + 0.110*vetsrising + 0.056*lunch + 0.056*hipster + 0.056*williamsburg + 0.056*photography + 0.003*vote5sos + 0.003*centralpark\n",
        "0.260*job + 0.208*jobs + 0.157*tweetmyjobs + 0.079*clerical + 0.079*selfie + 0.053*centralpark + 0.053*latergram + 0.001*photo + 0.001*competition + 0.001*design\n"
       ]
      }
     ],
     "prompt_number": 222
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def classify(lda_corpus, texts, cluster_num, threshold):\n",
      "    for i,j in zip(lda_corpus, get_tweet(twi_path)):\n",
      "            try: \n",
      "                if i[cluster_num][1] > threshold:\n",
      "                    yield j\n",
      "            except: pass"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 223
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Assigns the topics to the documents in corpus\n",
      "lda_corpus = lda[mm]\n",
      "\n",
      "# this just flattens the list of scores \n",
      "#scores = map(lambda score: score[1], lda_corpus)\n",
      "#threshold = sum(scores)/len(scores)\n",
      "\n",
      "threshold = 1/num_topics\n",
      "classifications = map(lambda number: list(classify(lda_corpus, texts, number, threshold)), np.arange(num_topics)+1)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 224
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print len(classifications)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "20\n"
       ]
      }
     ],
     "prompt_number": 225
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 225
    }
   ],
   "metadata": {}
  }
 ]
}