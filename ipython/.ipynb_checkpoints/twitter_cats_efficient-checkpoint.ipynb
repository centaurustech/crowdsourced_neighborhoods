{
 "metadata": {
  "name": "",
  "signature": "sha256:2148cabcf270a9b2170fbe25c123267496db81f6c4e43cfedee687af37064ccb"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from os import listdir\n",
      "from os.path import isfile, join\n",
      "from ttp import ttp\n",
      "from nltk.corpus import stopwords\n",
      "from gensim import corpora, models, similarities\n",
      "from itertools import chain\n",
      "import numpy as np\n",
      "\n",
      "import json\n",
      "import csv"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 63
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# hashtags related to NY are eliminated, since they add noise to the topic modelling analysis and don't contribute to our purpose\n",
      "new_yorks = ['nyc', 'newyork', 'manhattan', 'brooklin', 'new_york', 'new york', 'brooklyn', 'ny', 'newyorkcity', 'nuevayork', 'bk']\n",
      "\n",
      "# function that opens twitter parsed jsons on a given file path, and returns their hashtags, or a list of location, content, and ID                            \n",
      "def jsons_to_mm_tuple(twi_path, just_hashes=False):\n",
      "    # make sure only filenames are on the list of files\n",
      "    twi_jsons = [ f for f in listdir(twi_path) if isfile(join(twi_path,f)) ][:100]\n",
      "    \n",
      "    # initialize an empty list to store tweet ids\n",
      "    all_ids = []\n",
      "    for f in twi_jsons:\n",
      "        # open each json file\n",
      "        with open('%s%s' %(twi_path, f)) as json_file:\n",
      "            # load the json into a python dict\n",
      "            try:\n",
      "                tweets = json.load(json_file).values()\n",
      "                for tweet in tweets:\n",
      "                    content = tweet['content']\n",
      "                    p = ttp.Parser()\n",
      "                    # parse the content for hashtags: returns a list of hashtags\n",
      "                    parsed_tweet = p.parse(content)\n",
      "                    # hashtags should be lower case, and not include newyork or similar hastags\n",
      "                    hashtags = [tag.lower().encode('utf-8') for tag in parsed_tweet.tags if tag.lower().encode('utf-8') not in new_yorks]\n",
      "\n",
      "                    # we only process the jsons which actually have hashtags and geographical coordinates\n",
      "                    if len(hashtags) > 0:\n",
      "                        if float(tweet['lat']) != 0 and float(tweet['lon']) != 0:\n",
      "                            # we make sure that the tweets are not already in our list of tweets\n",
      "                            if tweet['tweet_id'] not in all_ids:\n",
      "                                # add the new tweet to the list of tweets\n",
      "                                all_ids.append(tweet['tweet_id'])\n",
      "\n",
      "                                # if we want to yield more information than just the hashtags\n",
      "                                if not just_hashes:\n",
      "                                    yield [tweet['tweet_id'], tweet['lat'], tweet['lon'], content, hashtags]\n",
      "                                else: \n",
      "                                    yield hashtags\n",
      "            except: pass"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 64
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# extract the tweets from the jsons\n",
      "twi_path = '../python/twitter/'\n",
      "tweets = jsons_to_mm_tuple(twi_path, True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 65
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# TOKENIZERATOR: collect statistics about all tokens\n",
      "dictionary = corpora.Dictionary(tweets)\n",
      "\n",
      "dictionary"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 66,
       "text": [
        "<gensim.corpora.dictionary.Dictionary at 0x18015fd0>"
       ]
      }
     ],
     "prompt_number": 66
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# get a list of stop words from the nltk library\n",
      "stoplist = stopwords.words('english')\n",
      "\n",
      "# DICTERATOR: remove stop words and words that appear only once \n",
      "stop_ids = [dictionary.token2id[stopword] for stopword in stoplist if stopword in dictionary.token2id]\n",
      "once_ids = [tokenid for tokenid, docfreq in dictionary.dfs.iteritems() if docfreq == 1]\n",
      "\n",
      "# filter the tokens from the corpora dict\n",
      "dictionary.filter_tokens(stop_ids + once_ids)\n",
      "dictionary"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 67,
       "text": [
        "<gensim.corpora.dictionary.Dictionary at 0x18015fd0>"
       ]
      }
     ],
     "prompt_number": 67
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# remove gaps in id sequence after words that were removed\n",
      "dictionary.compactify() \n",
      "dictionary"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 68,
       "text": [
        "<gensim.corpora.dictionary.Dictionary at 0x18015fd0>"
       ]
      }
     ],
     "prompt_number": 68
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# given a dictionary and a list of ids, get the words that correspond to the ids\n",
      "def get_singles(dictionary, ids):\n",
      "    for word_id in once_ids:\n",
      "        yield dictionary.get(word_id)\n",
      "\n",
      "# eliminate the words that appear only once        \n",
      "def filter_singles(singles, texts):\n",
      "    for text in texts:\n",
      "        new_list = []\n",
      "        for word in text:\n",
      "            if word not in singles:\n",
      "                new_list.append(word)\n",
      "        yield new_list      "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 69
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "singles = get_singles(dictionary, once_ids)\n",
      "filtered_texts = filter_singles(singles, jsons_to_mm_tuple(twi_path, True))\n",
      "\n",
      "# Create Bag of words\n",
      "mm = [dictionary.doc2bow(text) for text in filtered_texts] "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 70
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# define the number of topics for the classification\n",
      "num_topics = 40\n",
      "\n",
      "# Trains the LDA models with the corpus and dictionary previously created\n",
      "lda = models.ldamodel.LdaModel(corpus=list(mm), id2word=dictionary, num_topics=num_topics, \n",
      "                               update_every=1, chunksize=10000, passes=10, iterations=50)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 71
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# prints all groups and their main words\n",
      "lda.print_topics(num_topics=num_topics, num_words=15)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 72,
       "text": [
        "[u'0.308*halloween + 0.008*tweetmyjobs + 0.008*truth + 0.008*noschool + 0.008*art + 0.008*instagram + 0.008*competition + 0.008*design + 0.008*votefifthharmony + 0.008*cupcakes + 0.008*veteransday + 0.008*justsaying + 0.008*theview + 0.008*hipster + 0.008*smh',\n",
        " u'0.320*timessquare + 0.320*lunch + 0.004*sushi + 0.004*tweetmyjobs + 0.004*justsaying + 0.004*art + 0.004*instagram + 0.004*competition + 0.004*design + 0.004*votefifthharmony + 0.004*cupcakes + 0.004*veteransday + 0.004*theview + 0.004*empiresummit + 0.004*hipster',\n",
        " u'0.379*job + 0.306*jobs + 0.230*tweetmyjobs + 0.001*clerical + 0.001*photo + 0.001*noschool + 0.001*instagram + 0.001*competition + 0.001*design + 0.001*votefifthharmony + 0.001*cupcakes + 0.001*veteransday + 0.001*justsaying + 0.001*theview + 0.001*hipster',\n",
        " u'0.192*williamsburg + 0.192*photography + 0.192*hipster + 0.005*tweetmyjobs + 0.005*justsaying + 0.005*art + 0.005*instagram + 0.005*competition + 0.005*design + 0.005*votefifthharmony + 0.005*cupcakes + 0.005*veteransday + 0.005*theview + 0.005*empiresummit + 0.005*smh',\n",
        " u'0.196*justsaying + 0.196*hungry + 0.196*centralpark + 0.196*truth + 0.002*tweetmyjobs + 0.002*noschool + 0.002*art + 0.002*instagram + 0.002*competition + 0.002*design + 0.002*votefifthharmony + 0.002*cupcakes + 0.002*veteransday + 0.002*hipster + 0.002*theview',\n",
        " u'0.243*foodporn + 0.243*empiresummit + 0.123*datenight + 0.123*ilovemyjob + 0.003*justsaying + 0.003*noschool + 0.003*art + 0.003*instagram + 0.003*competition + 0.003*design + 0.003*votefifthharmony + 0.003*cupcakes + 0.003*veteransday + 0.003*makeup + 0.003*marketing',\n",
        " u'0.238*yamecanse + 0.198*yamecansedemurillokaram + 0.198*unionsquare + 0.119*vscocam + 0.080*vsco + 0.080*hilarious + 0.001*veteransday + 0.001*art + 0.001*instagram + 0.001*competition + 0.001*design + 0.001*votefifthharmony + 0.001*cupcakes + 0.001*theview + 0.001*justsaying',\n",
        " u'0.549*vote5sos + 0.140*noschool + 0.003*tweetmyjobs + 0.003*truth + 0.003*art + 0.003*instagram + 0.003*competition + 0.003*design + 0.003*votefifthharmony + 0.003*cupcakes + 0.003*veteransday + 0.003*justsaying + 0.003*theview + 0.003*hipster + 0.003*smh',\n",
        " u'0.674*somethingbigvideo + 0.105*downtown + 0.105*saturdaynight + 0.001*tweetmyjobs + 0.001*theview + 0.001*art + 0.001*instagram + 0.001*competition + 0.001*design + 0.001*votefifthharmony + 0.001*cupcakes + 0.001*veteransday + 0.001*justsaying + 0.001*hipster + 0.001*empiresummit',\n",
        " u'0.237*art + 0.237*nyfol + 0.006*tweetmyjobs + 0.006*theview + 0.006*instagram + 0.006*competition + 0.006*design + 0.006*votefifthharmony + 0.006*cupcakes + 0.006*veteransday + 0.006*justsaying + 0.006*hipster + 0.006*empiresummit + 0.006*smh + 0.006*ebola',\n",
        " u'0.320*vetsrising + 0.320*hometomama + 0.004*tweetmyjobs + 0.004*justsaying + 0.004*noschool + 0.004*art + 0.004*instagram + 0.004*competition + 0.004*design + 0.004*votefifthharmony + 0.004*cupcakes + 0.004*veteransday + 0.004*theview + 0.004*truth + 0.004*hipster',\n",
        " u'0.566*veteransday + 0.071*design + 0.071*infornyc + 0.071*igers + 0.071*collaboration + 0.036*noschool + 0.036*ilovemyjob + 0.001*tweetmyjobs + 0.001*justsaying + 0.001*art + 0.001*instagram + 0.001*competition + 0.001*votefifthharmony + 0.001*cupcakes + 0.001*hipster',\n",
        " u'0.291*clerical + 0.194*svengoolie + 0.194*party + 0.107*job + 0.002*tweetmyjobs + 0.002*veteransday + 0.002*noschool + 0.002*art + 0.002*instagram + 0.002*competition + 0.002*design + 0.002*votefifthharmony + 0.002*cupcakes + 0.002*justsaying + 0.002*truth',\n",
        " u'0.483*nyfol + 0.243*marketing + 0.003*tweetmyjobs + 0.003*theview + 0.003*art + 0.003*instagram + 0.003*competition + 0.003*design + 0.003*votefifthharmony + 0.003*cupcakes + 0.003*veteransday + 0.003*justsaying + 0.003*hipster + 0.003*empiresummit + 0.003*smh',\n",
        " u'0.217*emabestmalejustinbieber + 0.217*emabiggestfansjustinbieber + 0.110*instagram + 0.110*fall + 0.110*photography + 0.003*tweetmyjobs + 0.003*justsaying + 0.003*competition + 0.003*design + 0.003*votefifthharmony + 0.003*cupcakes + 0.003*veteransday + 0.003*theview + 0.003*noschool + 0.003*hipster',\n",
        " u'0.468*ebola + 0.006*tweetmyjobs + 0.006*theview + 0.006*noschool + 0.006*art + 0.006*instagram + 0.006*competition + 0.006*design + 0.006*votefifthharmony + 0.006*cupcakes + 0.006*veteransday + 0.006*justsaying + 0.006*hipster + 0.006*november + 0.006*smh',\n",
        " u'0.196*fkatwigs + 0.196*carlosbirthday + 0.196*newyorkdiaries + 0.099*vsco + 0.099*hipster + 0.002*tweetmyjobs + 0.002*instagram + 0.002*competition + 0.002*design + 0.002*votefifthharmony + 0.002*cupcakes + 0.002*veteransday + 0.002*justsaying + 0.002*theview + 0.002*ebola',\n",
        " u'0.267*fashion + 0.179*style + 0.179*makeup + 0.091*fall + 0.091*travel + 0.002*veteransday + 0.002*noschool + 0.002*art + 0.002*instagram + 0.002*competition + 0.002*design + 0.002*votefifthharmony + 0.002*cupcakes + 0.002*tweetmyjobs + 0.002*justsaying',\n",
        " u'0.413*canifffollowme + 0.276*sushi + 0.003*tweetmyjobs + 0.003*justsaying + 0.003*noschool + 0.003*art + 0.003*instagram + 0.003*competition + 0.003*design + 0.003*votefifthharmony + 0.003*cupcakes + 0.003*veteransday + 0.003*theview + 0.003*truth + 0.003*hipster',\n",
        " u'0.237*datenight + 0.237*nycomedyfestival + 0.006*art + 0.006*instagram + 0.006*competition + 0.006*design + 0.006*votefifthharmony + 0.006*cupcakes + 0.006*veteransday + 0.006*justsaying + 0.006*theview + 0.006*hipster + 0.006*empiresummit + 0.006*smh + 0.006*ebola',\n",
        " u'0.432*askjacob + 0.217*festivaloflights + 0.110*dumbo + 0.003*tweetmyjobs + 0.003*theview + 0.003*art + 0.003*instagram + 0.003*competition + 0.003*design + 0.003*votefifthharmony + 0.003*cupcakes + 0.003*veteransday + 0.003*justsaying + 0.003*hipster + 0.003*empiresummit',\n",
        " u'0.483*votefifthharmony + 0.243*dance + 0.003*tweetmyjobs + 0.003*hipster + 0.003*art + 0.003*instagram + 0.003*competition + 0.003*design + 0.003*cupcakes + 0.003*veteransday + 0.003*justsaying + 0.003*theview + 0.003*smh + 0.003*empiresummit + 0.003*ebola',\n",
        " u'0.320*handbags + 0.162*style + 0.162*dumbo + 0.004*tweetmyjobs + 0.004*theview + 0.004*art + 0.004*instagram + 0.004*competition + 0.004*design + 0.004*votefifthharmony + 0.004*cupcakes + 0.004*veteransday + 0.004*justsaying + 0.004*smh + 0.004*hipster',\n",
        " u'0.716*usa + 0.003*hipster + 0.003*art + 0.003*instagram + 0.003*competition + 0.003*design + 0.003*votefifthharmony + 0.003*cupcakes + 0.003*veteransday + 0.003*justsaying + 0.003*theview + 0.003*tweetmyjobs + 0.003*empiresummit + 0.003*smh + 0.003*ebola',\n",
        " u'0.483*hopkinskovalev + 0.243*smh + 0.003*makeup + 0.003*tweetmyjobs + 0.003*justsaying + 0.003*art + 0.003*instagram + 0.003*competition + 0.003*design + 0.003*votefifthharmony + 0.003*cupcakes + 0.003*veteransday + 0.003*hipster + 0.003*theview + 0.003*empiresummit',\n",
        " u'0.243*coffee + 0.243*photogrid + 0.243*cupcakes + 0.003*tweetmyjobs + 0.003*art + 0.003*instagram + 0.003*competition + 0.003*design + 0.003*votefifthharmony + 0.003*veteransday + 0.003*justsaying + 0.003*theview + 0.003*hipster + 0.003*empiresummit + 0.003*smh',\n",
        " u'0.267*theview + 0.179*nyfol + 0.179*repost + 0.179*dumbo + 0.002*tweetmyjobs + 0.002*art + 0.002*instagram + 0.002*competition + 0.002*design + 0.002*votefifthharmony + 0.002*cupcakes + 0.002*veteransday + 0.002*justsaying + 0.002*smh + 0.002*hipster',\n",
        " u'0.355*nycf + 0.179*totaltoptuesday + 0.179*funfearlesslife + 0.091*nycomedyfestival + 0.002*tweetmyjobs + 0.002*justsaying + 0.002*noschool + 0.002*art + 0.002*instagram + 0.002*competition + 0.002*design + 0.002*votefifthharmony + 0.002*cupcakes + 0.002*veteransday + 0.002*theview',\n",
        " u'0.324*love + 0.217*latergram + 0.217*connectedhome + 0.003*tweetmyjobs + 0.003*justsaying + 0.003*art + 0.003*instagram + 0.003*competition + 0.003*design + 0.003*votefifthharmony + 0.003*cupcakes + 0.003*veteransday + 0.003*hipster + 0.003*theview + 0.003*empiresummit',\n",
        " u'0.324*photo + 0.217*happyveteransday + 0.110*usa + 0.110*beautiful + 0.003*justsaying + 0.003*noschool + 0.003*art + 0.003*instagram + 0.003*competition + 0.003*design + 0.003*votefifthharmony + 0.003*cupcakes + 0.003*veteransday + 0.003*makeup + 0.003*marketing',\n",
        " u'0.237*beautiful + 0.237*theview + 0.006*noschool + 0.006*art + 0.006*instagram + 0.006*competition + 0.006*design + 0.006*votefifthharmony + 0.006*cupcakes + 0.006*veteransday + 0.006*justsaying + 0.006*tweetmyjobs + 0.006*empiresummit + 0.006*smh + 0.006*ebola',\n",
        " u'0.196*nov8 + 0.196*sheertalent + 0.196*competition + 0.196*beermenus + 0.002*justsaying + 0.002*noschool + 0.002*art + 0.002*instagram + 0.002*design + 0.002*votefifthharmony + 0.002*cupcakes + 0.002*veteransday + 0.002*tweetmyjobs + 0.002*empiresummit + 0.002*hipster',\n",
        " u'0.267*selfie + 0.179*november + 0.179*goodtimes + 0.179*onenewark + 0.002*hipster + 0.002*art + 0.002*instagram + 0.002*competition + 0.002*design + 0.002*votefifthharmony + 0.002*cupcakes + 0.002*veteransday + 0.002*justsaying + 0.002*theview + 0.002*ebola',\n",
        " u'0.468*steinerstudios + 0.006*tweetmyjobs + 0.006*theview + 0.006*noschool + 0.006*art + 0.006*instagram + 0.006*competition + 0.006*design + 0.006*votefifthharmony + 0.006*cupcakes + 0.006*veteransday + 0.006*justsaying + 0.006*hipster + 0.006*november + 0.006*smh',\n",
        " u'0.354*travel + 0.145*usa + 0.120*art + 0.120*halloween + 0.003*justsaying + 0.003*noschool + 0.003*instagram + 0.003*competition + 0.003*design + 0.003*votefifthharmony + 0.003*cupcakes + 0.003*veteransday + 0.003*tweetmyjobs + 0.003*truth + 0.003*theview',\n",
        " u'0.568*broadway + 0.005*tweetmyjobs + 0.005*hipster + 0.005*art + 0.005*instagram + 0.005*competition + 0.005*design + 0.005*votefifthharmony + 0.005*cupcakes + 0.005*veteransday + 0.005*justsaying + 0.005*theview + 0.005*smh + 0.005*empiresummit + 0.005*ebola',\n",
        " u'0.363*music + 0.243*foodie + 0.123*instagram + 0.003*justsaying + 0.003*noschool + 0.003*art + 0.003*competition + 0.003*design + 0.003*votefifthharmony + 0.003*cupcakes + 0.003*veteransday + 0.003*theview + 0.003*truth + 0.003*hipster + 0.003*smh',\n",
        " u'0.011*tweetmyjobs + 0.011*theview + 0.011*noschool + 0.011*art + 0.011*instagram + 0.011*competition + 0.011*design + 0.011*votefifthharmony + 0.011*cupcakes + 0.011*veteransday + 0.011*justsaying + 0.011*hipster + 0.011*november + 0.011*smh + 0.011*ebola',\n",
        " u'0.363*alavslsu + 0.363*murrayftw + 0.003*theview + 0.003*art + 0.003*instagram + 0.003*competition + 0.003*design + 0.003*votefifthharmony + 0.003*cupcakes + 0.003*veteransday + 0.003*justsaying + 0.003*hipster + 0.003*empiresummit + 0.003*smh + 0.003*ebola',\n",
        " u'0.308*williamsburg + 0.008*tweetmyjobs + 0.008*truth + 0.008*noschool + 0.008*art + 0.008*instagram + 0.008*competition + 0.008*design + 0.008*votefifthharmony + 0.008*cupcakes + 0.008*veteransday + 0.008*justsaying + 0.008*theview + 0.008*hipster + 0.008*smh']"
       ]
      }
     ],
     "prompt_number": 72
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "'''this should be replaced!!!!!!!!!!!!!!!'''\n",
      "lda.show_topics(num_topics=num_topics, num_words=1, formatted=False)\n",
      "test = [i[0][1] for i in lda.show_topics(num_topics=num_topics, num_words=1, formatted=False)]\n",
      "\n",
      "frequencies = np.arange(num_topics)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 104
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# get a list of processed topics obtained by training an LDA model, and return them as individual lists of topics and frewuencies\n",
      "def parse_topics(filepath):\n",
      "    with open(filepath, 'rU') as f:\n",
      "        reader = list(csv.reader(f))\n",
      "        header = reader[0]\n",
      "        reader.pop(0)\n",
      "        topics = []\n",
      "        freqs = []\n",
      "        for row in reader:\n",
      "            freq = []\n",
      "            topic = []\n",
      "            row.pop(0)\n",
      "            for ind, element in enumerate(row):\n",
      "                if ind%2 == 0:\n",
      "                    try: \n",
      "                        fr = row[ind+1]\n",
      "                    except: \n",
      "                        fr = ''\n",
      "                    if fr != '':\n",
      "                        topic.append(element)\n",
      "                        freq.append(row[ind+1])\n",
      "            topics.append(topic)\n",
      "            freqs.append(freq)\n",
      "        return topics, freqs      "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 172
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# given a corpus trained with the LDA classifier, and a threshold, classify the tweets into the groups \n",
      "def classify(lda_corpus, texts, cluster_num, threshold, words=None, frequencies=None):\n",
      "    for i,j in zip(lda_corpus, texts):\n",
      "        try: \n",
      "            if i[cluster_num][1] > threshold:\n",
      "                j.append(words[cluster_num])\n",
      "                j.append(frequencies[cluster_num])\n",
      "                yield j \n",
      "        except: pass"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 173
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Assigns the topics to the documents in corpus\n",
      "lda_corpus = lda[mm]\n",
      "threshold = 1/float(num_topics)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 174
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# function that takes the topic classification of a given tweet, and other data of the tweets and writes a new json to be spatially joined\n",
      "def topic_to_json(topic_num, topics, frequencies):\n",
      "    for tweet in classify(lda_corpus, jsons_to_mm_tuple(twi_path), topic_num, threshold, topics, frequencies):\n",
      "        tid, lat, lon, content, hashtags, topic, frequency = tweet\n",
      "        with open('json_topic/%stopic_tweet.json' %(tid), 'w') as f:\n",
      "            f.write( json.dumps({'lat':lat, 'lon':lon, 'tweet':content, 'hashtags':hashtags, 'topic':topic, 'frequency':frequency}))\n",
      "            #print 'wrote tweet %s' %(tid)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 175
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "topics, frequencies = parse_topics('twitter_40topic_classification.csv')\n",
      "\n",
      "# for every topic group, write json files for every tweet\n",
      "for topic_num in np.arange(num_topics):#lda_corpus, jsons_to_mm_tuple(twi_path), topic_num, threshold, num_topics):        \n",
      "    topic_to_json(topic_num, topics, frequencies)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 177
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 77
    }
   ],
   "metadata": {}
  }
 ]
}