{
 "metadata": {
  "name": "",
  "signature": "sha256:f535ed5f039f82d0437918df040663961283c390ce16e7591cac14318fd34a17"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from os import listdir\n",
      "from os.path import isfile, join\n",
      "from ttp import ttp\n",
      "from nltk.corpus import stopwords\n",
      "from gensim import corpora, models, similarities\n",
      "from itertools import chain\n",
      "import numpy as np\n",
      "\n",
      "import json\n",
      "import csv\n",
      "\n",
      "def get_tweet(twi_path):\n",
      "    twi_jsons = [ f for f in listdir(twi_path) if isfile(join(twi_path,f)) ][:100]\n",
      "\n",
      "    all_ids = []\n",
      "    for f in twi_jsons:\n",
      "        with open('%s%s' %(twi_path, f)) as json_file:\n",
      "            tweets = json.load(json_file).values()\n",
      "            for tweet in tweets:\n",
      "                content = tweet['content']\n",
      "\n",
      "                if float(tweet['lat']) != 0 and float(tweet['lon']) != 0:\n",
      "                    if tweet['tweet_id'] not in all_ids:\n",
      "                        all_ids.append(tweet['tweet_id'])\n",
      "                        yield tweet['content'].encode('utf-8')\n",
      "                        \n",
      "\n",
      "def parse_jsons(twi_path):\n",
      "    twi_jsons = [ f for f in listdir(twi_path) if isfile(join(twi_path,f)) ][:100]\n",
      "\n",
      "    all_ids = []\n",
      "    for f in twi_jsons:\n",
      "        with open('%s%s' %(twi_path, f)) as json_file:\n",
      "            tweets = json.load(json_file).values()\n",
      "            for tweet in tweets:\n",
      "                content = tweet['content']\n",
      "                p = ttp.Parser()\n",
      "                parsed_tweet = p.parse(content)\n",
      "                hashtags = [tag.encode('utf-8') for tag in parsed_tweet.tags]\n",
      "                if float(tweet['lat']) != 0 and float(tweet['lon']) != 0:\n",
      "                    if tweet['tweet_id'] not in all_ids:\n",
      "                        all_ids.append(tweet['tweet_id'])\n",
      "                        yield hashtags#tweet['content']#.encode('utf-8')\n",
      "    \n",
      "def make_texts(tweets):\n",
      "    stoplist = stopwords.words('english')\n",
      "    for tweet in tweets:\n",
      "        yield [word for word in tweet.lower().split() if word not in stoplist]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 220
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "twi_path = '../python/twitter/'\n",
      "tweets = parse_jsons(twi_path)\n",
      "texts = make_texts(tweets)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 185
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# TOKENIZERATOR: collect statistics about all tokens\n",
      "dictionary = corpora.Dictionary(tweets)\n",
      "\n",
      "print (dictionary)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Dictionary(872 unique tokens: [u'sustainabledesign', u'ThisCouldBeUsButYou', u'studenspecials', u'veteransday2014', u'fitfam']...)\n"
       ]
      }
     ],
     "prompt_number": 187
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "stoplist = stopwords.words('english')\n",
      "\n",
      "# DICTERATOR: remove stop words and words that appear only once \n",
      "stop_ids = [dictionary.token2id[stopword] for stopword in stoplist if stopword in dictionary.token2id]\n",
      "once_ids = [tokenid for tokenid, docfreq in dictionary.dfs.iteritems() if docfreq == 1]\n",
      "dictionary.filter_tokens(stop_ids + once_ids)\n",
      "print (dictionary)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Dictionary(100 unique tokens: [u'datenight', u'NewYork', u'photo', u'fall', u'goodtimes']...)\n"
       ]
      }
     ],
     "prompt_number": 188
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dictionary.compactify() # remove gaps in id sequence after words that were removed\n",
      "print dictionary"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Dictionary(100 unique tokens: [u'nyc', u'datenight', u'NewYork', u'photo', u'goodtimes']...)\n"
       ]
      }
     ],
     "prompt_number": 189
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_singles(dictionary, ids):\n",
      "    for word_id in once_ids:\n",
      "        yield dictionary.get(word_id)\n",
      "\n",
      "def filter_singles(singles, texts):\n",
      "    for text in texts:\n",
      "        new_list = []\n",
      "        for word in text:\n",
      "            if word not in singles:\n",
      "                new_list.append(word)\n",
      "        yield new_list\n",
      "        \n",
      "def mm_iterator(dictionary, texts):\n",
      "    for text in texts:\n",
      "        yield dictionary.doc2bow(text)\n",
      "                "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 190
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "singles = get_singles(dictionary, once_ids)\n",
      "filtered_texts = filter_singles(singles, parse_jsons(twi_path))\n",
      "# Create Bag of words\n",
      "mm = [dictionary.doc2bow(text) for text in parse_jsons(twi_path)]#mm_iterator(dictionary, filtered_texts)\n",
      "#other_mm = mm_iterator(dictionary, parse_jsons(twi_path))#[dictionary.doc2bow(text) for text in make_texts(parse_jsons(twi_path))]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 206
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "''' the corpus can be obtained from a saved file somewhere instead of memory'''\n",
      "num_topics = 20\n",
      "\n",
      "# Trains the LDA models.\n",
      "lda = models.ldamodel.LdaModel(corpus=list(mm), id2word=dictionary, num_topics=num_topics, update_every=1, chunksize=10000, passes=1)\n",
      "\n",
      "for i in lda.print_topics(): print i"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.179*Brooklyn + 0.062*nyc + 0.062*NYC + 0.062*music + 0.062*brooklyn + 0.062*centralpark + 0.062*truth + 0.062*vote5sos + 0.062*coffee + 0.062*justsaying\n",
        "0.161*Brooklyn + 0.161*NYC + 0.108*newyork + 0.055*williamsburg + 0.055*hipster + 0.055*photography + 0.055*veteransday + 0.055*hungry + 0.055*emabestmalejustinbieber + 0.003*nyc\n",
        "0.256*nyc + 0.065*NewYork + 0.065*Job + 0.065*selfie + 0.044*TweetMyJobs + 0.044*Jobs + 0.044*YaMeCanse + 0.044*YaMeCanseDeMurilloKaram + 0.044*unionsquare + 0.044*NEWYORK\n",
        "0.186*PhotoGrid + 0.095*nyc + 0.095*broadway + 0.095*ny + 0.095*canifffollowme + 0.005*NYC + 0.005*VeteransDay + 0.005*Brooklyn + 0.005*TheView + 0.005*vote5sos\n",
        "0.121*NYFOL + 0.062*Job + 0.062*Brooklyn + 0.062*NewYork + 0.062*Jobs + 0.062*TweetMyJobs + 0.062*party + 0.062*newyork + 0.062*love + 0.062*marketing\n",
        "0.152*NYC + 0.052*VeteransDay + 0.052*ny + 0.052*newyorkcity + 0.052*centralpark + 0.052*foodie + 0.052*vsco + 0.052*hipster + 0.052*askjacob + 0.052*halloween\n",
        "0.267*SomethingBigVideo + 0.090*NYC + 0.090*Brooklyn + 0.090*NYCF + 0.046*fkatwigs + 0.046*TheView + 0.046*HappyVeteransDay + 0.046*BeerMenus + 0.028*handbags + 0.026*brooklyn\n",
        "0.138*nyc + 0.104*USA + 0.070*manhattan + 0.036*photo + 0.036*YaMeCanse + 0.036*newyork + 0.036*unionsquare + 0.036*vsco + 0.036*NewYork + 0.036*usa\n",
        "0.081*Job + 0.081*Jobs + 0.081*travel + 0.081*style + 0.081*fashion + 0.081*TweetMyJobs + 0.081*marketing + 0.081*newyorkcity + 0.004*nyc + 0.004*NYC\n",
        "0.102*EmpireSummit + 0.053*nyc + 0.052*newyorkcity + 0.052*sushi + 0.052*justsaying + 0.052*truth + 0.052*canifffollowme + 0.052*VeteransDay + 0.052*MurrayFTW + 0.052*onenewark\n"
       ]
      }
     ],
     "prompt_number": 207
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def classify(lda_corpus, texts, cluster_num, threshold):\n",
      "    for i,j in zip(lda_corpus, get_tweet(twi_path)):\n",
      "            try: \n",
      "                if i[cluster_num][1] > threshold:\n",
      "                    yield j\n",
      "            except: pass"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 230
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Assigns the topics to the documents in corpus\n",
      "lda_corpus = lda[mm]\n",
      "\n",
      "# this just flattens the list of scores \n",
      "#scores = map(lambda score: score[1], lda_corpus)\n",
      "#threshold = sum(scores)/len(scores)\n",
      "\n",
      "threshold = 1/num_topics\n",
      "classifications = map(lambda number: list(classify(lda_corpus, texts, number, threshold)), np.arange(num_topics)+1)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 231
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 193
    }
   ],
   "metadata": {}
  }
 ]
}